{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import transformers\n",
    "import bert_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Zero: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "cased_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "uncased_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Hi, my name is bert [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(['Hi, my name is bert'])\n",
    "tokenizer.decode(tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] colleges å¤© largest happened smile donation [SEP]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = cased_tokenizer(['Hi, my name is bert'])\n",
    "cased_tokenizer.decode(tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pretrained_bert = bert_tests.get_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7632, 1010, 2026, 2171, 2003, 14324, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 103, 102]], 'token_type_ids': [[0, 0, 0]], 'attention_mask': [[1, 1, 1]]}\n",
      "{'input_ids': [[101, 1109, 1783, 18062, 8474, 1108, 4331, 3999, 103, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tokens = cased_tokenizer(['The firetruck was painted bright [MASK].'])\n",
    "\n",
    "mask_tokens = cased_tokenizer(['[MASK]'])\n",
    "\n",
    "probabilities = pretrained_bert.eval()(t.tensor(tokens['input_ids'])).logits.softmax(dim=1)\n",
    "\n",
    "mask_id = mask_tokens[\"input_ids\"][0][1]\n",
    "\n",
    "print(mask_tokens)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('red', 0.5602872371673584),\n",
       " ('yellow', 0.12122748047113419),\n",
       " ('white', 0.07108743488788605),\n",
       " ('blue', 0.069109246134758),\n",
       " ('green', 0.05439625307917595)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mask(tokens):\n",
    "    token_numbers = t.tensor(tokens[\"input_ids\"])\n",
    "    mask_index = (token_numbers == mask_id).nonzero(as_tuple = True)[1]\n",
    "    probabilities = pretrained_bert.eval()(t.tensor(tokens['input_ids'])).logits.softmax(dim=2)\n",
    "\n",
    "    topkprobs, topktokens = probabilities.topk(5, dim=2)\n",
    "\n",
    "    topkprobs = topkprobs[0,mask_index,:]\n",
    "    topktokens = topktokens[0,mask_index,:]\n",
    "\n",
    "    topkguesses = cased_tokenizer.decode(topktokens[0]).split(' ')\n",
    "\n",
    "    return zip(topkguesses, topkprobs.tolist()[0])\n",
    "\n",
    "list(get_mask(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The British Prime Minister is ___.\n",
      "                          17% elected\n",
      "                          12% British\n",
      "                          10% Hon\n",
      "                           9% appointed\n",
      "                           4% Conservative\n",
      "The capital of Illinois is ___.\n",
      "                       43% Chicago\n",
      "                       20% Springfield\n",
      "                        2% Cairo\n",
      "                        1% Madison\n",
      "                        1% Quincy\n",
      "17 plus 80 equals ___.\n",
      "               4% 10\n",
      "               4% 16\n",
      "               4% 20\n",
      "               3% 15\n",
      "               3% 12\n",
      "1, 1, 2, 3, 5, 6, ___.\n",
      "              43% 7\n",
      "              15% 8\n",
      "               5% 10\n",
      "               4% 12\n",
      "               4% 6\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(string):\n",
    "    tokens = cased_tokenizer([string])\n",
    "    guesses = get_mask(tokens)\n",
    "\n",
    "    indent = string.index('[MASK]')\n",
    "\n",
    "    string = string.replace('[MASK]', '___')\n",
    "\n",
    "    print(string)\n",
    "\n",
    "    for word, prob in guesses:\n",
    "        prob_string = \"{}%\".format(round(prob * 100)) \n",
    "        # print(prob_string)\n",
    "        prob_string = \" \" * (indent - len(prob_string) - 1) + prob_string + \" \" + word\n",
    "\n",
    "        print(prob_string)\n",
    "\n",
    "# ascii_art_probs('The firetruck was painted bright [MASK].')\n",
    "# ascii_art_probs('The fish loves to eat [MASK].')\n",
    "# ascii_art_probs('The fish loves to eat [MASK]')\n",
    "ascii_art_probs('The British Prime Minister is [MASK].')\n",
    "ascii_art_probs('The capital of Illinois is [MASK].')\n",
    "\n",
    "ascii_art_probs('17 plus 80 equals [MASK].')\n",
    "\n",
    "ascii_art_probs('1, 1, 2, 3, 5, [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[[0.7243, 0.7186, 0.6659, 0.6492, 0.6321],\n",
       "         [0.4452, 0.4371, 0.4316, 0.4289, 0.4204],\n",
       "         [0.5841, 0.5715, 0.5522, 0.5305, 0.5068],\n",
       "         [0.9977, 0.9940, 0.9618, 0.9562, 0.9484],\n",
       "         [0.9658, 0.9607, 0.9572, 0.9535, 0.9525],\n",
       "         [1.0000, 0.9999, 0.9999, 0.9988, 0.9987],\n",
       "         [1.0000, 1.0000, 0.9971, 0.9967, 0.9965],\n",
       "         [0.9999, 0.9993, 0.9963, 0.9912, 0.9899],\n",
       "         [0.9482, 0.9479, 0.9333, 0.9326, 0.9138],\n",
       "         [0.9845, 0.9832, 0.9801, 0.9770, 0.9696],\n",
       "         [0.9527, 0.9515, 0.9405, 0.9300, 0.9293]]], grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[[ 2870,  3398,  2932, 19230, 10163],\n",
       "         [23764,  1368,  1333,  1371,  1381],\n",
       "         [ 2543, 26566,  9363, 13836, 23630],\n",
       "         [16344, 10845, 12514, 10497,  6703],\n",
       "         [22071, 24243, 12722, 27525, 15514],\n",
       "         [ 2001,  2495, 14557, 19569, 16920],\n",
       "         [ 4993, 14429, 28236,  3715,  2118],\n",
       "         [ 4408, 20292, 12301,  6455,  5260],\n",
       "         [25651,  3220, 13657, 25211, 24783],\n",
       "         [  635,   106,   197,   636,   136],\n",
       "         [ 6844, 11943, 13003, 16943, 14135]]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = probabilities.topk(5, dim=2)\n",
    "\n",
    "topk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two: Fine Tuning on Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
