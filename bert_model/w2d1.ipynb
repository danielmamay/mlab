{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import transformers\n",
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: -0.01152 STD: 0.1016 VALS [0.03246 0.09426 -0.07523 0.106 0.006123 0.1217 0.1085 0.06477 0.05628 -0.1145...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    K = rearrange(project_key(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "    Q = rearrange(project_query(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "\n",
    "    KbyQ = t.einsum(\"bsnh,btnh -> bnst\", K, Q)\n",
    "\n",
    "    d_k = token_activations.shape[2]/num_heads\n",
    "\n",
    "    out = KbyQ/t.sqrt(t.tensor([d_k]))\n",
    "    return out\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0024 STD: 0.1174 VALS [-0.08224 0.05514 0.02298 -0.08445 -0.03101 0.06312 -0.1459 -0.09142 -0.04613 -0.04548...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    s = t.nn.Softmax(dim=2)\n",
    "\n",
    "    out = s(attention_pattern) # batch_size, head_num, key_token, query_token\n",
    "\n",
    "    out = rearrange(out, \"b n k q -> b n q k 1\")\n",
    "\n",
    "    V = rearrange(project_value(token_activations), \"b k (n h) -> b n 1 k h\", n = num_heads) # batch_size, num_heads, 1, key, head_size\n",
    "\n",
    "    out = einsum(\"bnqkh,bnqkh -> bnqh\", out, V)\n",
    "\n",
    "    out = rearrange(out, \"b n q h -> b q (n h)\")\n",
    "\n",
    "    return project_output(out)\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super(MultiHeadedSelfAttention, self).__init__()\n",
    "\n",
    "        hidden_dim = num_heads * hidden_size\n",
    "\n",
    "        self.query = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = t.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        attention_scores = raw_attention_pattern(input, self.num_heads, self.query, self.key)\n",
    "\n",
    "        attention = bert_attention(input, self.num_heads, attention_scores, self.value, self.output)\n",
    "\n",
    "        return attention\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    out = linear_1(token_activations)\n",
    "    out = t.nn.GELU()(out)\n",
    "    return linear_2(out)\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(t.nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size):\n",
    "        super(BertMLP, self).__init__()\n",
    "        self.linear_1 = t.nn.Linear(input_size,intermediate_size)\n",
    "        self.linear_2 = t.nn.Linear(intermediate_size,input_size)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        return bert_mlp(input,self.linear_1,self.linear_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -9.537e-09 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(t.nn.Module):\n",
    "    def __init__(self, normalized_dim):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,)))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,)))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        eps = 1e-05\n",
    "        mean = input.mean(-1).unsqueeze(-1)\n",
    "        mean.detach()\n",
    "        stdev = input.std(-1,unbiased = False).unsqueeze(-1)\n",
    "        stdev.detach()\n",
    "        out = (input - mean)/t.sqrt(stdev**2 + eps) \n",
    "        return out*self.weight + self.bias\n",
    "bert_tests.test_layer_norm(LayerNorm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -2.484e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.layer_norm1 = LayerNorm(hidden_size)\n",
    "        self.mlp = BertMLP(hidden_size, intermediate_size)\n",
    "        self.layer_norm2 = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.mhsa(input)\n",
    "        out = self.layer_norm1(input + out)\n",
    "        residual = out\n",
    "        out = self.mlp(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_norm2(residual + out)\n",
    "        return out\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(['Hello, I am a sentence.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embedding = t.nn.Parameter(t.randn((vocab_size, embed_size)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.embedding[input]\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(input_ids, token_type_ids, position_embedding, token_embedding, token_type_embedding, layer_norm, dropout):        \n",
    "    positions = repeat(t.arange(input_ids.shape[1]), \"p -> b p\", b = input_ids.shape[0])\n",
    "    \n",
    "    if input_ids.is_cuda:\n",
    "        device = input_ids.get_device()\n",
    "        positions.to(device = device)\n",
    "    \n",
    "    out = token_embedding(input_ids) + token_type_embedding(token_type_ids) + position_embedding(positions)\n",
    "    out = layer_norm(out)\n",
    "    return dropout(out)\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 2.69e-09 STD: 1 VALS [-0.009385 -0.4919 0.9852 -0.3535 -3.624 1.333 1.163 1.449 1.063 0.246...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = t.zeros_like(input_ids)\n",
    "        return bert_embedding(input_ids, token_type_ids, self.position_embedding, self.token_embedding, self.token_type_embedding, self.layer_norm, self.dropout)\n",
    "\n",
    "bert_tests.test_bert_embedding(BertEmbedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout, intermediate_size, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = t.nn.Sequential(\n",
    "            BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout),\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)],\n",
    "            t.nn.Linear(hidden_size, hidden_size),\n",
    "            t.nn.GELU(),\n",
    "            LayerNorm(hidden_size),\n",
    "            t.nn.Linear(hidden_size, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        return self.model(input_ids)\n",
    "\n",
    "bert_tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-classification out torch.Size([1, 768])\n",
      "Post-classification out torch.Size([1, 2])\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    }
   ],
   "source": [
    "class BertClassification(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout, intermediate_size, num_heads, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = t.nn.Sequential(*[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.lmhead = t.nn.Sequential(\n",
    "            t.nn.Linear(hidden_size, hidden_size),\n",
    "            t.nn.GELU(),\n",
    "            LayerNorm(hidden_size)\n",
    "        )\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "        self.unembedding = t.nn.Linear(hidden_size, vocab_size)\n",
    "        self.classification_head = t.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        out = self.embedding(input_ids)\n",
    "\n",
    "        out = self.transformer(out)\n",
    "\n",
    "        logits = self.lmhead(out)\n",
    "        logits = self.unembedding(logits)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = out[:,0,:]\n",
    "\n",
    "        print('Pre-classification out', out.shape)\n",
    "\n",
    "        out = self.classification_head(out)\n",
    "\n",
    "        print('Post-classification out', out.shape)\n",
    "\n",
    "        return (logits, out)\n",
    "\n",
    "bert_tests.test_bert_classification(BertClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = BertClassification(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12, num_classes = 2,\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding.layer_norm.weight',\n",
       " 'embedding.layer_norm.bias',\n",
       " 'transformer.0.layer_norm.weight',\n",
       " 'transformer.0.layer_norm.bias',\n",
       " 'transformer.0.residual.layer_norm.weight',\n",
       " 'transformer.0.residual.layer_norm.bias',\n",
       " 'transformer.1.layer_norm.weight',\n",
       " 'transformer.1.layer_norm.bias',\n",
       " 'transformer.1.residual.layer_norm.weight',\n",
       " 'transformer.1.residual.layer_norm.bias',\n",
       " 'transformer.2.layer_norm.weight',\n",
       " 'transformer.2.layer_norm.bias',\n",
       " 'transformer.2.residual.layer_norm.weight',\n",
       " 'transformer.2.residual.layer_norm.bias',\n",
       " 'transformer.3.layer_norm.weight',\n",
       " 'transformer.3.layer_norm.bias',\n",
       " 'transformer.3.residual.layer_norm.weight',\n",
       " 'transformer.3.residual.layer_norm.bias',\n",
       " 'transformer.4.layer_norm.weight',\n",
       " 'transformer.4.layer_norm.bias',\n",
       " 'transformer.4.residual.layer_norm.weight',\n",
       " 'transformer.4.residual.layer_norm.bias',\n",
       " 'transformer.5.layer_norm.weight',\n",
       " 'transformer.5.layer_norm.bias',\n",
       " 'transformer.5.residual.layer_norm.weight',\n",
       " 'transformer.5.residual.layer_norm.bias',\n",
       " 'transformer.6.layer_norm.weight',\n",
       " 'transformer.6.layer_norm.bias',\n",
       " 'transformer.6.residual.layer_norm.weight',\n",
       " 'transformer.6.residual.layer_norm.bias',\n",
       " 'transformer.7.layer_norm.weight',\n",
       " 'transformer.7.layer_norm.bias',\n",
       " 'transformer.7.residual.layer_norm.weight',\n",
       " 'transformer.7.residual.layer_norm.bias',\n",
       " 'transformer.8.layer_norm.weight',\n",
       " 'transformer.8.layer_norm.bias',\n",
       " 'transformer.8.residual.layer_norm.weight',\n",
       " 'transformer.8.residual.layer_norm.bias',\n",
       " 'transformer.9.layer_norm.weight',\n",
       " 'transformer.9.layer_norm.bias',\n",
       " 'transformer.9.residual.layer_norm.weight',\n",
       " 'transformer.9.residual.layer_norm.bias',\n",
       " 'transformer.10.layer_norm.weight',\n",
       " 'transformer.10.layer_norm.bias',\n",
       " 'transformer.10.residual.layer_norm.weight',\n",
       " 'transformer.10.residual.layer_norm.bias',\n",
       " 'transformer.11.layer_norm.weight',\n",
       " 'transformer.11.layer_norm.bias',\n",
       " 'transformer.11.residual.layer_norm.weight',\n",
       " 'transformer.11.residual.layer_norm.bias',\n",
       " 'lm_head.layer_norm.weight',\n",
       " 'lm_head.layer_norm.bias']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def mapkey(key):\n",
    "\n",
    "    # embedding = model.0\n",
    "    # key = re.sub(r'^embedding', 'bert.model.0', key)\n",
    "\n",
    "    # transformer.i = model.(i+1)\n",
    "    # key = re.sub(r'transformer\\.(\\d+)', lambda expr: \"bert.model.{}\".format(int(expr.groups()[0]) + 1), key)\n",
    "\n",
    "    # attention = mhsa\n",
    "    key = re.sub('.attention.', '.mhsa.', key)\n",
    "\n",
    "    key = re.sub('lm_head.mlp', 'lmhead.0', key)\n",
    "    key = re.sub('lm_head.unembedding', 'unembedding', key)\n",
    "    key = re.sub('lm_head.layer_norm', 'lmhead.2', key)\n",
    "\n",
    "    key = re.sub('_embedding.weight', '_embedding.embedding', key)\n",
    "\n",
    "    key = re.sub(r'pattern\\.project_(key|query)', lambda expr: \"{}\".format(expr.groups()[0]), key)\n",
    "\n",
    "    key = re.sub('project_out', 'output', key)\n",
    "    key = re.sub('project_value', 'value', key)\n",
    "\n",
    "    key = re.sub(r'residual\\.mlp(1|2)', lambda expr: \"mlp.linear_{}\".format(expr.groups()[0]), key)\n",
    "\n",
    "    # key = re.sub(r'^((?!residual).)*\\.layer_norm', 'layer_norm1', key)\n",
    "    key = re.sub(r'residual\\.layer_norm', 'layer_norm2', key)\n",
    "    key = re.sub(r'layer_norm\\.', 'layer_norm1.', key)\n",
    "\n",
    "    key = re.sub(r'embedding\\.layer_norm1', 'embedding.layer_norm', key)\n",
    "\n",
    "    # key = re.sub(r'model\\.15', 'model.temp', key)\n",
    "    # key = re.sub(r'model\\.16', 'model.15', key)\n",
    "    # key = re.sub(r'model\\.temp', 'model.16',key)\n",
    "\n",
    "    key = re.sub(r'^model', \"transformer\", key)\n",
    "\n",
    "\n",
    "    return key\n",
    "\n",
    "mapkey('embedding gesgse')\n",
    "mapkey('transformer.1')\n",
    "mapkey('.attention.')\n",
    "\n",
    "# False 15 model.1.mlp.linear_1.weight model.1.residual.mlp1.weight\n",
    "# False 16 model.1.mlp.linear_1.bias model.1.residual.mlp1.bias\n",
    "\n",
    "\n",
    "def matching(xs, pattern):\n",
    "    return [s for s in xs if re.search(pattern, s)]\n",
    "\n",
    "matching(list(pretrained_bert.state_dict()), 'layer_norm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "pretrained_params = list(pretrained_bert.state_dict())\n",
    "\n",
    "# for i, our_param in enumerate(my_bert.state_dict()):\n",
    "#     print(our_param == mapkey(pretrained_params[i]), i, our_param, mapkey(pretrained_params[i]))\n",
    "\n",
    "for pretrained_param in pretrained_bert.state_dict():\n",
    "    if mapkey(pretrained_param) not in my_bert.state_dict():\n",
    "        print(mapkey(pretrained_param))\n",
    "\n",
    "# for i, our_param in enumerate(my_bert.state_dict()):\n",
    "#     print(our_param, pretrained_params[i])\n",
    "\n",
    "\n",
    "# print(list(pretrained_bert.state_dict()))\n",
    "# print(list(my_bert.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "for pretrained_param in pretrained_bert.state_dict():\n",
    "    new_state_dict[mapkey(pretrained_param)] = pretrained_bert.state_dict()[pretrained_param]\n",
    "\n",
    "my_bert.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-classification out torch.Size([10, 768])\n",
      "Post-classification out torch.Size([10, 2])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "allclose(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-07ecf228b657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_same_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_bert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mlab/days/w2d1/bert_tests.py\u001b[0m in \u001b[0;36mtest_same_output\u001b[0;34m(your_bert, pretrained_bert, tol)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     allclose(your_bert.eval()(input_ids),\n\u001b[0m\u001b[1;32m    203\u001b[0m              \u001b[0mpretrained_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m              'comparing Berts', tol=tol)\n",
      "\u001b[0;32m~/mlab/days/w2d1/bert_tests.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(my_out, their_out, name, tol)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheir_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheir_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0merrstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'error in {name}\\n{tpeek(\"\", my_out, ret=True)} \\n!=\\n{tpeek(\"\", their_out, ret=True)}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: allclose(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning on Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "data_train = torchtext.datasets.IMDB(root='.data',split='train')\n",
    "data_test = torchtext.datasets.IMDB(root='.data',split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = []\n",
    "for d in data_train:\n",
    "    dataset_train.append(d)\n",
    "\n",
    "dataset_test = []\n",
    "for d in data_test:\n",
    "    dataset_test.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_batches(dataset, batch_size=4, max_seq_length=512):\n",
    "    # data = []\n",
    "    # for d in dataset:\n",
    "    #     data.append(d)    \n",
    "\n",
    "    dataset.sort(key=lambda x: len(x[1]))\n",
    "\n",
    "    number_of_batches = math.ceil(len(dataset) / batch_size)\n",
    "\n",
    "    dataset = np.array_split(dataset, number_of_batches)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for batch in dataset:\n",
    "        scores, reviews = zip(*batch)\n",
    "\n",
    "        tokens = tokenizer(list(reviews))\n",
    "\n",
    "        # print(len(tokens['input_ids'][0]))\n",
    "        tokens = tokens[\"input_ids\"]\n",
    "\n",
    "        tokens = [review_token[:max_seq_length] for review_token in tokens]\n",
    "        tokens = [review_token if len(review_token) == max_seq_length else review_token + (max_seq_length - len(review_token))*[tokenizer.pad_token_id] for review_token in tokens]\n",
    "\n",
    "        X = t.tensor(tokens)\n",
    "        y = t.tensor([1 if s == \"pos\" else 0 for s in scores])\n",
    "\n",
    "        batches.append((X,y))\n",
    "\n",
    "    random.shuffle(batches)\n",
    "    return batches\n",
    "\n",
    "\n",
    "# print(len(extract_batches(dataset_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[ 0.3361, -0.0086],\n",
      "        [ 0.3001, -0.0810],\n",
      "        [ 0.3455,  0.1134],\n",
      "        [ 0.1969, -0.2294]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5853, 0.4147],\n",
      "        [0.5941, 0.4059],\n",
      "        [0.5578, 0.4422],\n",
      "        [0.6050, 0.3950]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[0.0097, 0.2153],\n",
      "        [0.1315, 0.0425],\n",
      "        [0.1771, 0.2815],\n",
      "        [0.5024, 0.2732]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4488, 0.5512],\n",
      "        [0.5222, 0.4778],\n",
      "        [0.4739, 0.5261],\n",
      "        [0.5571, 0.4429]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.1237,  0.1334],\n",
      "        [ 0.2886, -0.1136],\n",
      "        [ 0.4495, -0.1559],\n",
      "        [ 0.3398, -0.1486]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4976, 0.5024],\n",
      "        [0.5992, 0.4008],\n",
      "        [0.6469, 0.3531],\n",
      "        [0.6197, 0.3803]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.0155,  0.2162],\n",
      "        [ 0.7037,  0.2338],\n",
      "        [ 0.1840, -0.3121],\n",
      "        [ 0.2209, -0.1891]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4500, 0.5500],\n",
      "        [0.6154, 0.3846],\n",
      "        [0.6215, 0.3785],\n",
      "        [0.6011, 0.3989]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.1155,  0.1363],\n",
      "        [-0.1049,  0.2805],\n",
      "        [ 0.0229,  0.3825],\n",
      "        [-0.4162,  0.4363]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4374, 0.5626],\n",
      "        [0.4048, 0.5952],\n",
      "        [0.4111, 0.5889],\n",
      "        [0.2989, 0.7011]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.4228,  0.5043],\n",
      "        [-0.5696,  0.6603],\n",
      "        [-0.5032,  0.4653],\n",
      "        [-0.5096,  0.3802]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2835, 0.7165],\n",
      "        [0.2262, 0.7738],\n",
      "        [0.2752, 0.7248],\n",
      "        [0.2911, 0.7089]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[-0.2319,  0.6073],\n",
      "        [-0.0366,  0.1443],\n",
      "        [-0.0917,  0.3885],\n",
      "        [-0.3573,  0.4347]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3017, 0.6983],\n",
      "        [0.4549, 0.5451],\n",
      "        [0.3822, 0.6178],\n",
      "        [0.3118, 0.6882]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.0682,  0.2518],\n",
      "        [ 0.3157,  0.2060],\n",
      "        [-0.0956,  0.5895],\n",
      "        [ 0.0919, -0.1479]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4542, 0.5458],\n",
      "        [0.5274, 0.4726],\n",
      "        [0.3351, 0.6649],\n",
      "        [0.5597, 0.4403]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[-0.2410,  0.4665],\n",
      "        [ 0.1979,  0.5439],\n",
      "        [-0.0929,  0.2561],\n",
      "        [ 0.3979,  0.0823]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3301, 0.6699],\n",
      "        [0.4144, 0.5856],\n",
      "        [0.4136, 0.5864],\n",
      "        [0.5782, 0.4218]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[-0.5266,  0.3775],\n",
      "        [-0.4904,  0.4997],\n",
      "        [-0.1444,  0.4156],\n",
      "        [-0.4960,  0.5010]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2882, 0.7118],\n",
      "        [0.2709, 0.7291],\n",
      "        [0.3636, 0.6364],\n",
      "        [0.2695, 0.7305]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "10\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-0.1080,  0.2111],\n",
      "        [ 0.0870,  0.0661],\n",
      "        [-0.1462,  0.4569],\n",
      "        [ 0.0153,  0.4350]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4209, 0.5791],\n",
      "        [0.5052, 0.4948],\n",
      "        [0.3536, 0.6464],\n",
      "        [0.3966, 0.6034]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 0], device='cuda:0') tensor([[0.1643, 0.0846],\n",
      "        [0.0521, 0.2818],\n",
      "        [0.1092, 0.2713],\n",
      "        [0.0773, 0.2033]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5199, 0.4801],\n",
      "        [0.4428, 0.5572],\n",
      "        [0.4596, 0.5404],\n",
      "        [0.4686, 0.5314]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[-0.1473,  0.2111],\n",
      "        [ 0.3085,  0.0560],\n",
      "        [ 0.2045,  0.4463],\n",
      "        [-0.1661,  0.3283]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4113, 0.5887],\n",
      "        [0.5628, 0.4372],\n",
      "        [0.4399, 0.5601],\n",
      "        [0.3789, 0.6211]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0759,  0.0205],\n",
      "        [-0.0752,  0.4488],\n",
      "        [-0.2924,  0.4832],\n",
      "        [-0.2052,  0.6311]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4759, 0.5241],\n",
      "        [0.3719, 0.6281],\n",
      "        [0.3153, 0.6847],\n",
      "        [0.3023, 0.6977]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[-0.5332,  0.4437],\n",
      "        [-0.2011,  0.6639],\n",
      "        [-0.2167,  0.4784],\n",
      "        [ 0.2506,  0.6444]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2735, 0.7265],\n",
      "        [0.2963, 0.7037],\n",
      "        [0.3329, 0.6671],\n",
      "        [0.4028, 0.5972]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 1], device='cuda:0') tensor([[-0.2434,  0.3961],\n",
      "        [-0.2690,  0.1804],\n",
      "        [-0.0976,  0.4062],\n",
      "        [-0.3225,  0.1217]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3454, 0.6546],\n",
      "        [0.3895, 0.6105],\n",
      "        [0.3766, 0.6234],\n",
      "        [0.3907, 0.6093]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.1274,  0.1772],\n",
      "        [-0.3186,  0.0583],\n",
      "        [ 0.4012,  0.5442],\n",
      "        [-0.0408, -0.0585]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4875, 0.5125],\n",
      "        [0.4069, 0.5931],\n",
      "        [0.4643, 0.5357],\n",
      "        [0.5044, 0.4956]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[ 0.1150,  0.0692],\n",
      "        [-0.0343, -0.0233],\n",
      "        [ 0.2018, -0.0349],\n",
      "        [ 0.1103, -0.0703]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5115, 0.4885],\n",
      "        [0.4972, 0.5028],\n",
      "        [0.5589, 0.4411],\n",
      "        [0.5450, 0.4550]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 1, 1], device='cuda:0') tensor([[ 0.0917, -0.0594],\n",
      "        [-0.0150,  0.0960],\n",
      "        [ 0.0020, -0.0037],\n",
      "        [ 0.0723,  0.2350]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5377, 0.4623],\n",
      "        [0.4723, 0.5277],\n",
      "        [0.5014, 0.4986],\n",
      "        [0.4594, 0.5406]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.0529,  0.1876],\n",
      "        [-0.1706,  0.1638],\n",
      "        [-0.0899,  0.3573],\n",
      "        [-0.2299,  0.0924]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4664, 0.5336],\n",
      "        [0.4172, 0.5828],\n",
      "        [0.3900, 0.6100],\n",
      "        [0.4201, 0.5799]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "20\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.1651,  0.3709],\n",
      "        [-0.1903,  0.1942],\n",
      "        [-0.1080,  0.3367],\n",
      "        [-0.3721,  0.1530]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4487, 0.5513],\n",
      "        [0.4051, 0.5949],\n",
      "        [0.3906, 0.6094],\n",
      "        [0.3717, 0.6283]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.1833,  0.0324],\n",
      "        [-0.1252,  0.0927],\n",
      "        [-0.2838,  0.0613],\n",
      "        [-0.0026,  0.4762]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4463, 0.5537],\n",
      "        [0.4457, 0.5543],\n",
      "        [0.4146, 0.5854],\n",
      "        [0.3825, 0.6175]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 1, 1], device='cuda:0') tensor([[-0.4086,  0.4878],\n",
      "        [ 0.0068,  0.3540],\n",
      "        [-0.1501,  0.4732],\n",
      "        [-0.4476,  0.2957]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2898, 0.7102],\n",
      "        [0.4140, 0.5860],\n",
      "        [0.3490, 0.6510],\n",
      "        [0.3223, 0.6777]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[ 0.1031,  0.0284],\n",
      "        [-0.1453,  0.1336],\n",
      "        [ 0.1616,  0.2175],\n",
      "        [-0.1739,  0.1292]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5187, 0.4813],\n",
      "        [0.4307, 0.5693],\n",
      "        [0.4860, 0.5140],\n",
      "        [0.4248, 0.5752]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0746,  0.2168],\n",
      "        [-0.1691,  0.1665],\n",
      "        [ 0.3603,  0.1108],\n",
      "        [-0.0292,  0.1313]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4277, 0.5723],\n",
      "        [0.4169, 0.5831],\n",
      "        [0.5621, 0.4379],\n",
      "        [0.4600, 0.5400]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-0.2014,  0.2699],\n",
      "        [-0.2685,  0.1696],\n",
      "        [-0.3279,  0.2776],\n",
      "        [ 0.0512,  0.0462]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3843, 0.6157],\n",
      "        [0.3922, 0.6078],\n",
      "        [0.3531, 0.6469],\n",
      "        [0.5012, 0.4988]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.1114,  0.1101],\n",
      "        [-0.2114,  0.2384],\n",
      "        [-0.0283,  0.3469],\n",
      "        [-0.1286,  0.0998]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4449, 0.5551],\n",
      "        [0.3894, 0.6106],\n",
      "        [0.4073, 0.5927],\n",
      "        [0.4432, 0.5568]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 0, 1], device='cuda:0') tensor([[-0.1395,  0.3775],\n",
      "        [-0.1486,  0.1993],\n",
      "        [ 0.0323,  0.4339],\n",
      "        [-0.0607,  0.5668]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3736, 0.6264],\n",
      "        [0.4139, 0.5861],\n",
      "        [0.4009, 0.5991],\n",
      "        [0.3481, 0.6519]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[-0.0783,  0.5578],\n",
      "        [-0.3596,  0.4648],\n",
      "        [ 0.1899,  0.0254],\n",
      "        [ 0.0902,  0.2341]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3461, 0.6539],\n",
      "        [0.3048, 0.6952],\n",
      "        [0.5410, 0.4590],\n",
      "        [0.4641, 0.5359]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[0.0367, 0.1031],\n",
      "        [0.2920, 0.0932],\n",
      "        [0.2751, 0.0934],\n",
      "        [0.0911, 0.3190]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4834, 0.5166],\n",
      "        [0.5495, 0.4505],\n",
      "        [0.5453, 0.4547],\n",
      "        [0.4433, 0.5567]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "30\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.4723, -0.1611],\n",
      "        [ 0.0276,  0.1461],\n",
      "        [-0.0918,  0.0595],\n",
      "        [ 0.2752,  0.3002]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6532, 0.3468],\n",
      "        [0.4704, 0.5296],\n",
      "        [0.4622, 0.5378],\n",
      "        [0.4938, 0.5062]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.0586,  0.2684],\n",
      "        [-0.0713,  0.1510],\n",
      "        [ 0.0537,  0.0990],\n",
      "        [-0.0318,  0.1245]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4190, 0.5810],\n",
      "        [0.4447, 0.5553],\n",
      "        [0.4887, 0.5113],\n",
      "        [0.4610, 0.5390]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[-0.2666,  0.5022],\n",
      "        [-0.1262,  0.2083],\n",
      "        [-0.0277,  0.4692],\n",
      "        [ 0.0601,  0.2757]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3167, 0.6833],\n",
      "        [0.4172, 0.5828],\n",
      "        [0.3783, 0.6217],\n",
      "        [0.4463, 0.5537]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[-0.0394,  0.4810],\n",
      "        [-0.1150,  0.2988],\n",
      "        [-0.2915,  0.6280],\n",
      "        [ 0.0824,  0.8040]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3728, 0.6272],\n",
      "        [0.3980, 0.6020],\n",
      "        [0.2851, 0.7149],\n",
      "        [0.3270, 0.6730]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 1], device='cuda:0') tensor([[-0.0093,  0.3514],\n",
      "        [-0.1190,  0.2563],\n",
      "        [-0.2472,  0.2873],\n",
      "        [ 0.0798,  0.6507]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4108, 0.5892],\n",
      "        [0.4073, 0.5927],\n",
      "        [0.3694, 0.6306],\n",
      "        [0.3610, 0.6390]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0879,  1.0191],\n",
      "        [-0.8197,  0.9581],\n",
      "        [-0.3802,  0.6678],\n",
      "        [-0.3529,  0.5750]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2484, 0.7516],\n",
      "        [0.1446, 0.8554],\n",
      "        [0.2596, 0.7404],\n",
      "        [0.2834, 0.7166]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-0.4290,  0.7047],\n",
      "        [-0.1300,  0.9457],\n",
      "        [-0.1556,  0.8070],\n",
      "        [-0.3074,  0.5585]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2435, 0.7565],\n",
      "        [0.2543, 0.7457],\n",
      "        [0.2764, 0.7236],\n",
      "        [0.2961, 0.7039]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[-0.3285,  0.4980],\n",
      "        [-0.2506,  0.2368],\n",
      "        [-0.0565,  0.7066],\n",
      "        [-0.2105,  0.2144]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3044, 0.6956],\n",
      "        [0.3805, 0.6195],\n",
      "        [0.3180, 0.6820],\n",
      "        [0.3953, 0.6047]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-0.1268,  0.2785],\n",
      "        [-0.0374,  0.1490],\n",
      "        [ 0.1247,  0.1625],\n",
      "        [ 0.1435,  0.2139]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4000, 0.6000],\n",
      "        [0.4535, 0.5465],\n",
      "        [0.4906, 0.5094],\n",
      "        [0.4824, 0.5176]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.4810,  0.5707],\n",
      "        [ 0.1221,  0.3810],\n",
      "        [ 0.3062,  0.3939],\n",
      "        [-0.1836,  0.3266]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2589, 0.7411],\n",
      "        [0.4356, 0.5644],\n",
      "        [0.4781, 0.5219],\n",
      "        [0.3751, 0.6249]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "40\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.1010,  0.6379],\n",
      "        [-0.2875,  0.5794],\n",
      "        [-0.3824,  0.7628],\n",
      "        [-0.0809,  0.6777]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3689, 0.6311],\n",
      "        [0.2959, 0.7041],\n",
      "        [0.2414, 0.7586],\n",
      "        [0.3190, 0.6810]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.3212,  0.3683],\n",
      "        [ 0.0222,  0.2926],\n",
      "        [ 0.1437,  0.2821],\n",
      "        [ 0.1968,  0.7099]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3342, 0.6658],\n",
      "        [0.4328, 0.5672],\n",
      "        [0.4654, 0.5346],\n",
      "        [0.3745, 0.6255]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.0203,  0.5067],\n",
      "        [-0.1898,  0.5707],\n",
      "        [-0.1376,  0.4789],\n",
      "        [-0.2843,  0.4071]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3712, 0.6288],\n",
      "        [0.3185, 0.6815],\n",
      "        [0.3506, 0.6494],\n",
      "        [0.3337, 0.6663]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[-0.0453,  0.1866],\n",
      "        [ 0.1605,  0.1995],\n",
      "        [ 0.2276,  0.2811],\n",
      "        [ 0.6068,  0.0267]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4423, 0.5577],\n",
      "        [0.4903, 0.5097],\n",
      "        [0.4866, 0.5134],\n",
      "        [0.6411, 0.3589]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.2787, -0.1026],\n",
      "        [-0.0099,  0.2979],\n",
      "        [-0.1438,  0.2049],\n",
      "        [ 0.1076,  0.0413]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5942, 0.4058],\n",
      "        [0.4237, 0.5763],\n",
      "        [0.4137, 0.5863],\n",
      "        [0.5166, 0.4834]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[-0.1016,  0.0621],\n",
      "        [ 0.2015,  0.1903],\n",
      "        [ 0.0541,  0.0938],\n",
      "        [ 0.2418,  0.0269]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4592, 0.5408],\n",
      "        [0.5028, 0.4972],\n",
      "        [0.4901, 0.5099],\n",
      "        [0.5535, 0.4465]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.3059, -0.0250],\n",
      "        [ 0.1743,  0.0489],\n",
      "        [ 0.0342,  0.0631],\n",
      "        [ 0.0487, -0.0124]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5820, 0.4180],\n",
      "        [0.5313, 0.4687],\n",
      "        [0.4928, 0.5072],\n",
      "        [0.5153, 0.4847]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[-0.1548,  0.1138],\n",
      "        [-0.0734,  0.1128],\n",
      "        [ 0.0710,  0.1593],\n",
      "        [-0.0064,  0.0668]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4333, 0.5667],\n",
      "        [0.4536, 0.5464],\n",
      "        [0.4779, 0.5221],\n",
      "        [0.4817, 0.5183]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[-0.0203,  0.2092],\n",
      "        [ 0.1841,  0.2395],\n",
      "        [ 0.3646,  0.3019],\n",
      "        [-0.1839,  0.2220]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4429, 0.5571],\n",
      "        [0.4862, 0.5138],\n",
      "        [0.5157, 0.4843],\n",
      "        [0.3999, 0.6001]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[0.1618, 0.0152],\n",
      "        [0.3052, 0.0685],\n",
      "        [0.0012, 0.0734],\n",
      "        [0.1277, 0.3040]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5366, 0.4634],\n",
      "        [0.5589, 0.4411],\n",
      "        [0.4820, 0.5180],\n",
      "        [0.4560, 0.5440]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "50\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.0635, -0.0410],\n",
      "        [-0.0056, -0.0183],\n",
      "        [ 0.5168,  0.3717],\n",
      "        [ 0.2061,  0.0824]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5261, 0.4739],\n",
      "        [0.5032, 0.4968],\n",
      "        [0.5362, 0.4638],\n",
      "        [0.5309, 0.4691]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 1], device='cuda:0') tensor([[ 0.2369, -0.3691],\n",
      "        [ 0.1548, -0.0984],\n",
      "        [ 0.1463,  0.0724],\n",
      "        [ 0.3511, -0.0296]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6470, 0.3530],\n",
      "        [0.5630, 0.4370],\n",
      "        [0.5185, 0.4815],\n",
      "        [0.5940, 0.4060]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.4239, -0.1481],\n",
      "        [ 0.2279, -0.2103],\n",
      "        [ 0.1373,  0.0068],\n",
      "        [ 0.0914, -0.3391]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6392, 0.3608],\n",
      "        [0.6078, 0.3922],\n",
      "        [0.5326, 0.4674],\n",
      "        [0.6060, 0.3940]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.0069, -0.0177],\n",
      "        [ 0.1221, -0.2034],\n",
      "        [ 0.0141,  0.1401],\n",
      "        [ 0.3571, -0.4182]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5027, 0.4973],\n",
      "        [0.5806, 0.4194],\n",
      "        [0.4685, 0.5315],\n",
      "        [0.6847, 0.3153]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.4134, -0.4174],\n",
      "        [ 0.0586, -0.3087],\n",
      "        [ 0.1585, -0.1161],\n",
      "        [ 0.3576, -0.1586]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6965, 0.3035],\n",
      "        [0.5908, 0.4092],\n",
      "        [0.5682, 0.4318],\n",
      "        [0.6263, 0.3737]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 0], device='cuda:0') tensor([[ 0.2095,  0.1285],\n",
      "        [ 0.2748, -0.1830],\n",
      "        [ 0.3076,  0.1446],\n",
      "        [ 0.3179,  0.2271]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5202, 0.4798],\n",
      "        [0.6125, 0.3875],\n",
      "        [0.5407, 0.4593],\n",
      "        [0.5227, 0.4773]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.2604, -0.3294],\n",
      "        [ 0.2044, -0.2401],\n",
      "        [-0.1505, -0.2629],\n",
      "        [ 0.1176, -0.2163]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6433, 0.3567],\n",
      "        [0.6093, 0.3907],\n",
      "        [0.5281, 0.4719],\n",
      "        [0.5827, 0.4173]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.2928, -0.1181],\n",
      "        [ 0.1428, -0.0695],\n",
      "        [ 0.2027, -0.2697],\n",
      "        [ 0.3134, -0.5519]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6013, 0.3987],\n",
      "        [0.5529, 0.4471],\n",
      "        [0.6160, 0.3840],\n",
      "        [0.7038, 0.2962]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.4144, -0.0818],\n",
      "        [ 0.4257, -0.3419],\n",
      "        [ 0.4027, -0.2144],\n",
      "        [ 0.0659, -0.4506]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6216, 0.3784],\n",
      "        [0.6830, 0.3170],\n",
      "        [0.6496, 0.3504],\n",
      "        [0.6263, 0.3737]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[ 0.2797, -0.0132],\n",
      "        [ 0.2001, -0.2025],\n",
      "        [ 0.3500, -0.2284],\n",
      "        [ 0.2993,  0.0610]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5727, 0.4273],\n",
      "        [0.5993, 0.4007],\n",
      "        [0.6407, 0.3593],\n",
      "        [0.5593, 0.4407]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "60\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[ 0.0337,  0.0880],\n",
      "        [-0.0986,  0.4979],\n",
      "        [ 0.1371, -0.0914],\n",
      "        [ 0.0236,  0.2796]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4864, 0.5136],\n",
      "        [0.3551, 0.6449],\n",
      "        [0.5569, 0.4431],\n",
      "        [0.4363, 0.5637]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.1586, -0.4443],\n",
      "        [ 0.3653, -0.2585],\n",
      "        [ 0.2099, -0.1673],\n",
      "        [ 0.1745, -0.1559]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6463, 0.3537],\n",
      "        [0.6511, 0.3489],\n",
      "        [0.5932, 0.4068],\n",
      "        [0.5819, 0.4181]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.0817,  0.1402],\n",
      "        [-0.2380,  0.0060],\n",
      "        [ 0.3623, -0.0604],\n",
      "        [ 0.0069, -0.0440]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4448, 0.5552],\n",
      "        [0.4393, 0.5607],\n",
      "        [0.6041, 0.3959],\n",
      "        [0.5127, 0.4873]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[-0.0295,  0.5083],\n",
      "        [-0.1952,  0.3985],\n",
      "        [-0.0849,  0.4619],\n",
      "        [-0.2003,  0.3429]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3687, 0.6313],\n",
      "        [0.3558, 0.6442],\n",
      "        [0.3666, 0.6334],\n",
      "        [0.3675, 0.6325]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[ 0.1491,  0.2690],\n",
      "        [ 0.2337,  0.0986],\n",
      "        [-0.0620,  0.3137],\n",
      "        [ 0.0761,  0.3230]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4700, 0.5300],\n",
      "        [0.5337, 0.4663],\n",
      "        [0.4072, 0.5928],\n",
      "        [0.4386, 0.5614]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.3337e-01, -2.3711e-01],\n",
      "        [ 1.5654e-01, -1.0542e-01],\n",
      "        [ 2.6963e-01, -1.6539e-01],\n",
      "        [ 1.1498e-01, -3.5887e-05]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5916, 0.4084],\n",
      "        [0.5651, 0.4349],\n",
      "        [0.6071, 0.3929],\n",
      "        [0.5287, 0.4713]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 1], device='cuda:0') tensor([[-0.2430,  0.3736],\n",
      "        [ 0.2551,  0.1892],\n",
      "        [ 0.2283,  0.1611],\n",
      "        [ 0.0751,  0.4604]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3506, 0.6494],\n",
      "        [0.5165, 0.4835],\n",
      "        [0.5168, 0.4832],\n",
      "        [0.4048, 0.5952]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.1059,  0.0656],\n",
      "        [ 0.1699, -0.0175],\n",
      "        [ 0.2885, -0.1188],\n",
      "        [ 0.1020, -0.1461]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4572, 0.5428],\n",
      "        [0.5467, 0.4533],\n",
      "        [0.6004, 0.3996],\n",
      "        [0.5617, 0.4383]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.1679,  0.0426],\n",
      "        [ 0.1751, -0.1253],\n",
      "        [ 0.3868, -0.0878],\n",
      "        [ 0.5312, -0.3523]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5313, 0.4687],\n",
      "        [0.5745, 0.4255],\n",
      "        [0.6165, 0.3835],\n",
      "        [0.7075, 0.2925]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.1029, -0.0150],\n",
      "        [ 0.2120, -0.0279],\n",
      "        [ 0.1874,  0.2994],\n",
      "        [-0.0478,  0.0714]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4780, 0.5220],\n",
      "        [0.5597, 0.4403],\n",
      "        [0.4720, 0.5280],\n",
      "        [0.4702, 0.5298]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "70\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.3741,  0.2251],\n",
      "        [ 0.0290, -0.1122],\n",
      "        [ 0.0437, -0.0913],\n",
      "        [ 0.2444, -0.1328]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5372, 0.4628],\n",
      "        [0.5353, 0.4647],\n",
      "        [0.5337, 0.4663],\n",
      "        [0.5932, 0.4068]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 0], device='cuda:0') tensor([[ 0.4886, -0.1143],\n",
      "        [ 0.2271,  0.1126],\n",
      "        [ 0.0510,  0.1081],\n",
      "        [ 0.2429, -0.1123]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6463, 0.3537],\n",
      "        [0.5286, 0.4714],\n",
      "        [0.4857, 0.5143],\n",
      "        [0.5879, 0.4121]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 1], device='cuda:0') tensor([[-0.0246,  0.0698],\n",
      "        [-0.1694,  0.1109],\n",
      "        [-0.0307, -0.1459],\n",
      "        [-0.1418,  0.3842]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4764, 0.5236],\n",
      "        [0.4304, 0.5696],\n",
      "        [0.5288, 0.4712],\n",
      "        [0.3714, 0.6286]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0176,  0.1184],\n",
      "        [ 0.3991,  0.0373],\n",
      "        [ 0.1705,  0.3459],\n",
      "        [-0.0788,  0.0812]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4661, 0.5339],\n",
      "        [0.5895, 0.4105],\n",
      "        [0.4562, 0.5438],\n",
      "        [0.4601, 0.5399]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 0], device='cuda:0') tensor([[ 0.0806,  0.1042],\n",
      "        [ 0.0840,  0.0247],\n",
      "        [-0.0300,  0.0729],\n",
      "        [-0.0024, -0.0053]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4941, 0.5059],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.4743, 0.5257],\n",
      "        [0.5007, 0.4993]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[ 0.2645,  0.3507],\n",
      "        [-0.0278,  0.0403],\n",
      "        [ 0.0280,  0.2159],\n",
      "        [-0.1364,  0.5245]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4785, 0.5215],\n",
      "        [0.4830, 0.5170],\n",
      "        [0.4532, 0.5468],\n",
      "        [0.3405, 0.6595]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.1052,  0.1978],\n",
      "        [-0.0863,  0.1639],\n",
      "        [ 0.2001, -0.0005],\n",
      "        [-0.1325,  0.3033]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4248, 0.5752],\n",
      "        [0.4378, 0.5622],\n",
      "        [0.5500, 0.4500],\n",
      "        [0.3927, 0.6073]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[ 0.0684,  0.1850],\n",
      "        [-0.0762,  0.2570],\n",
      "        [ 0.1905,  0.1410],\n",
      "        [ 0.2594,  0.2212]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4709, 0.5291],\n",
      "        [0.4175, 0.5825],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5095, 0.4905]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.0398,  0.3464],\n",
      "        [-0.1211,  0.6182],\n",
      "        [-0.0816,  0.6305],\n",
      "        [-0.1512,  0.3259]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4046, 0.5954],\n",
      "        [0.3231, 0.6769],\n",
      "        [0.3291, 0.6709],\n",
      "        [0.3829, 0.6171]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.1218,  0.2622],\n",
      "        [-0.2695,  0.1598],\n",
      "        [-0.2551,  0.0861],\n",
      "        [ 0.0205,  0.0987]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4052, 0.5948],\n",
      "        [0.3943, 0.6057],\n",
      "        [0.4155, 0.5845],\n",
      "        [0.4805, 0.5195]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "80\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.1022,  0.1719],\n",
      "        [ 0.2028, -0.0900],\n",
      "        [-0.0463,  0.3178],\n",
      "        [ 0.2056,  0.2283]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4319, 0.5681],\n",
      "        [0.5727, 0.4273],\n",
      "        [0.4100, 0.5900],\n",
      "        [0.4943, 0.5057]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0767,  0.2257],\n",
      "        [ 0.1009,  0.1116],\n",
      "        [-0.1121,  0.2194],\n",
      "        [ 0.3943,  0.3832]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4250, 0.5750],\n",
      "        [0.4973, 0.5027],\n",
      "        [0.4179, 0.5821],\n",
      "        [0.5028, 0.4972]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[ 0.3567, -0.3517],\n",
      "        [ 0.1394,  0.0035],\n",
      "        [ 0.2180, -0.3577],\n",
      "        [ 0.2407, -0.3592]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6700, 0.3300],\n",
      "        [0.5339, 0.4661],\n",
      "        [0.6401, 0.3599],\n",
      "        [0.6456, 0.3544]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 0, 1], device='cuda:0') tensor([[ 0.4959, -0.0229],\n",
      "        [ 0.2851, -0.1947],\n",
      "        [ 0.4387, -0.3747],\n",
      "        [ 0.4088, -0.3212]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6269, 0.3731],\n",
      "        [0.6177, 0.3823],\n",
      "        [0.6928, 0.3072],\n",
      "        [0.6748, 0.3252]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.7170, -0.2071],\n",
      "        [ 0.4689, -0.3283],\n",
      "        [ 0.3416,  0.0625],\n",
      "        [ 0.4529,  0.0570]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7159, 0.2841],\n",
      "        [0.6894, 0.3106],\n",
      "        [0.5693, 0.4307],\n",
      "        [0.5977, 0.4023]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.5256, -0.1477],\n",
      "        [ 0.3571, -0.1120],\n",
      "        [ 0.6798, -0.2315],\n",
      "        [ 0.1669, -0.1657]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6623, 0.3377],\n",
      "        [0.6152, 0.3848],\n",
      "        [0.7133, 0.2867],\n",
      "        [0.5824, 0.4176]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.0452,  0.1311],\n",
      "        [ 0.1488, -0.0768],\n",
      "        [ 0.1854,  0.0783],\n",
      "        [ 0.4073, -0.0399]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4560, 0.5440],\n",
      "        [0.5562, 0.4438],\n",
      "        [0.5267, 0.4733],\n",
      "        [0.6100, 0.3900]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 0.6311, -0.6906],\n",
      "        [ 0.7904, -0.4486],\n",
      "        [ 0.5033, -0.5422],\n",
      "        [ 0.6679, -0.4859]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7895, 0.2105],\n",
      "        [0.7754, 0.2246],\n",
      "        [0.7399, 0.2601],\n",
      "        [0.7602, 0.2398]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 1, 1], device='cuda:0') tensor([[ 0.8578, -0.4327],\n",
      "        [ 0.8867, -0.4306],\n",
      "        [ 0.5935, -0.6021],\n",
      "        [ 0.8698, -0.6599]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7842, 0.2158],\n",
      "        [0.7887, 0.2113],\n",
      "        [0.7677, 0.2323],\n",
      "        [0.8220, 0.1780]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.3832,  0.0857],\n",
      "        [-0.0258, -0.0914],\n",
      "        [ 0.1643, -0.0990],\n",
      "        [ 0.1037, -0.3421]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5738, 0.4262],\n",
      "        [0.5164, 0.4836],\n",
      "        [0.5654, 0.4346],\n",
      "        [0.6097, 0.3903]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "90\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.6772, -0.2587],\n",
      "        [ 0.4501, -0.3817],\n",
      "        [ 0.6363, -0.6345],\n",
      "        [ 0.6949, -0.2783]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7183, 0.2817],\n",
      "        [0.6967, 0.3033],\n",
      "        [0.7809, 0.2191],\n",
      "        [0.7258, 0.2742]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.9801, -0.5041],\n",
      "        [ 0.7334, -0.3761],\n",
      "        [ 0.6156, -0.5305],\n",
      "        [ 0.5275, -0.4677]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8152, 0.1848],\n",
      "        [0.7520, 0.2480],\n",
      "        [0.7588, 0.2412],\n",
      "        [0.7301, 0.2699]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[ 0.2085,  0.0778],\n",
      "        [ 0.1960, -0.0993],\n",
      "        [ 0.2146,  0.2453],\n",
      "        [ 0.2867, -0.0539]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5326, 0.4674],\n",
      "        [0.5733, 0.4267],\n",
      "        [0.4923, 0.5077],\n",
      "        [0.5843, 0.4157]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.2896, -0.1685],\n",
      "        [ 0.2945, -0.4578],\n",
      "        [ 0.6040, -0.6328],\n",
      "        [ 0.6302, -0.2309]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6126, 0.3874],\n",
      "        [0.6797, 0.3203],\n",
      "        [0.7750, 0.2250],\n",
      "        [0.7029, 0.2971]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 1, 0], device='cuda:0') tensor([[ 0.1946, -0.1232],\n",
      "        [ 0.2734, -0.0677],\n",
      "        [ 0.1269,  0.0940],\n",
      "        [ 0.2400,  0.0172]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5788, 0.4212],\n",
      "        [0.5845, 0.4155],\n",
      "        [0.5082, 0.4918],\n",
      "        [0.5555, 0.4445]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.6615, -0.4870],\n",
      "        [ 0.7615, -0.3190],\n",
      "        [ 0.8513, -0.6576],\n",
      "        [ 0.7198, -0.4742]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7592, 0.2408],\n",
      "        [0.7466, 0.2534],\n",
      "        [0.8189, 0.1811],\n",
      "        [0.7675, 0.2325]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.7198, -0.5929],\n",
      "        [ 0.4659, -0.4001],\n",
      "        [ 0.5929, -0.4149],\n",
      "        [ 0.7833, -0.4590]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7880, 0.2120],\n",
      "        [0.7039, 0.2961],\n",
      "        [0.7326, 0.2674],\n",
      "        [0.7760, 0.2240]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.6696, -0.4329],\n",
      "        [ 0.7689, -0.7446],\n",
      "        [ 0.5756, -0.4915],\n",
      "        [ 0.8617, -0.6679]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7507, 0.2493],\n",
      "        [0.8196, 0.1804],\n",
      "        [0.7440, 0.2560],\n",
      "        [0.8220, 0.1780]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.6968, -0.4722],\n",
      "        [ 0.8228, -0.6892],\n",
      "        [ 0.6047, -0.6651],\n",
      "        [ 0.7696, -0.6163]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7630, 0.2370],\n",
      "        [0.8194, 0.1806],\n",
      "        [0.7807, 0.2193],\n",
      "        [0.7999, 0.2001]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[ 0.8311, -0.7488],\n",
      "        [ 0.5239, -0.7475],\n",
      "        [ 0.5879, -0.4355],\n",
      "        [ 0.5602, -0.8160]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8292, 0.1708],\n",
      "        [0.7810, 0.2190],\n",
      "        [0.7356, 0.2644],\n",
      "        [0.7984, 0.2016]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "100\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.5540, -0.4331],\n",
      "        [ 0.5917, -0.5501],\n",
      "        [ 0.6637, -0.6251],\n",
      "        [ 0.6771, -0.5544]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7285, 0.2715],\n",
      "        [0.7580, 0.2420],\n",
      "        [0.7839, 0.2161],\n",
      "        [0.7741, 0.2259]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[ 0.7117, -0.3694],\n",
      "        [ 0.5063, -0.2033],\n",
      "        [ 0.2414, -0.2187],\n",
      "        [ 0.4679, -0.1029]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7467, 0.2533],\n",
      "        [0.6703, 0.3297],\n",
      "        [0.6130, 0.3870],\n",
      "        [0.6389, 0.3611]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 0], device='cuda:0') tensor([[ 0.4949, -0.5723],\n",
      "        [ 0.7680, -0.4882],\n",
      "        [ 0.6078, -0.5130],\n",
      "        [ 0.4112, -0.6077]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7441, 0.2559],\n",
      "        [0.7784, 0.2216],\n",
      "        [0.7541, 0.2459],\n",
      "        [0.7348, 0.2652]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.6982, -0.0276],\n",
      "        [ 0.4575, -0.2163],\n",
      "        [ 0.5217, -0.1692],\n",
      "        [ 0.3791, -0.1101]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6739, 0.3261],\n",
      "        [0.6623, 0.3377],\n",
      "        [0.6662, 0.3338],\n",
      "        [0.6199, 0.3801]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.2030, -0.2552],\n",
      "        [ 0.7126, -0.3013],\n",
      "        [ 0.2831, -0.4745],\n",
      "        [ 0.4232, -0.3859]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6126, 0.3874],\n",
      "        [0.7338, 0.2662],\n",
      "        [0.6808, 0.3192],\n",
      "        [0.6919, 0.3081]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.2417, -0.2847],\n",
      "        [ 0.4684, -0.3949],\n",
      "        [ 0.4603, -0.3452],\n",
      "        [ 0.4784, -0.4590]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6286, 0.3714],\n",
      "        [0.7033, 0.2967],\n",
      "        [0.6911, 0.3089],\n",
      "        [0.7186, 0.2814]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.4613, -0.1092],\n",
      "        [ 0.3172, -0.4486],\n",
      "        [ 0.5489, -0.2975],\n",
      "        [ 0.5080, -0.3696]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6389, 0.3611],\n",
      "        [0.6826, 0.3174],\n",
      "        [0.6998, 0.3002],\n",
      "        [0.7063, 0.2937]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 0.4282, -0.4159],\n",
      "        [ 0.2846, -0.4112],\n",
      "        [ 0.3092, -0.3717],\n",
      "        [ 0.2581, -0.3269]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6993, 0.3007],\n",
      "        [0.6672, 0.3328],\n",
      "        [0.6639, 0.3361],\n",
      "        [0.6422, 0.3578]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.5140, -0.1926],\n",
      "        [ 0.3036, -0.2236],\n",
      "        [ 0.2616, -0.1573],\n",
      "        [ 0.4894, -0.3603]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6696, 0.3304],\n",
      "        [0.6288, 0.3712],\n",
      "        [0.6032, 0.3968],\n",
      "        [0.7005, 0.2995]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 1], device='cuda:0') tensor([[ 0.4103, -0.2318],\n",
      "        [ 0.7072, -0.2462],\n",
      "        [ 0.2238, -0.2625],\n",
      "        [ 0.4676, -0.3715]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6552, 0.3448],\n",
      "        [0.7218, 0.2782],\n",
      "        [0.6192, 0.3808],\n",
      "        [0.6983, 0.3017]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "110\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.6410, -0.6780],\n",
      "        [ 0.7608, -0.3536],\n",
      "        [ 0.5062, -0.4985],\n",
      "        [ 0.4549, -0.1552]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7890, 0.2110],\n",
      "        [0.7529, 0.2471],\n",
      "        [0.7320, 0.2680],\n",
      "        [0.6479, 0.3521]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 0, 0], device='cuda:0') tensor([[ 0.7025, -0.6270],\n",
      "        [ 0.4037, -0.1256],\n",
      "        [ 0.4409, -0.4046],\n",
      "        [ 0.3007, -0.2624]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7908, 0.2092],\n",
      "        [0.6293, 0.3707],\n",
      "        [0.6996, 0.3004],\n",
      "        [0.6372, 0.3628]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.2908, -0.1344],\n",
      "        [ 0.0749,  0.0493],\n",
      "        [ 0.1285,  0.0047],\n",
      "        [ 0.4221,  0.1848]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6047, 0.3953],\n",
      "        [0.5064, 0.4936],\n",
      "        [0.5309, 0.4691],\n",
      "        [0.5590, 0.4410]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.2532,  0.2238],\n",
      "        [-0.1183,  0.3219],\n",
      "        [-0.0527,  0.0391],\n",
      "        [ 0.2682,  0.1219]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5073, 0.4927],\n",
      "        [0.3917, 0.6083],\n",
      "        [0.4771, 0.5229],\n",
      "        [0.5365, 0.4635]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.2829, -0.0442],\n",
      "        [ 0.2132,  0.1719],\n",
      "        [ 0.4652, -0.1540],\n",
      "        [ 0.1847,  0.1257]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5810, 0.4190],\n",
      "        [0.5103, 0.4897],\n",
      "        [0.6500, 0.3500],\n",
      "        [0.5147, 0.4853]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.5122, -0.0155],\n",
      "        [ 0.2167,  0.3426],\n",
      "        [ 0.3599,  0.0201],\n",
      "        [ 0.4142,  0.0192]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6289, 0.3711],\n",
      "        [0.4685, 0.5315],\n",
      "        [0.5841, 0.4159],\n",
      "        [0.5975, 0.4025]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 1], device='cuda:0') tensor([[ 0.0539, -0.0029],\n",
      "        [ 0.2185, -0.0907],\n",
      "        [ 0.3059,  0.1082],\n",
      "        [ 0.0636, -0.0669]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5142, 0.4858],\n",
      "        [0.5767, 0.4233],\n",
      "        [0.5492, 0.4508],\n",
      "        [0.5326, 0.4674]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[0.1929, 0.3843],\n",
      "        [0.1074, 0.2514],\n",
      "        [0.0638, 0.1257],\n",
      "        [0.0436, 0.4658]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4523, 0.5477],\n",
      "        [0.4641, 0.5359],\n",
      "        [0.4845, 0.5155],\n",
      "        [0.3960, 0.6040]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.0734, -0.0049],\n",
      "        [-0.1310,  0.2810],\n",
      "        [-0.0847,  0.2096],\n",
      "        [ 0.0854, -0.0126]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5196, 0.4804],\n",
      "        [0.3984, 0.6016],\n",
      "        [0.4269, 0.5731],\n",
      "        [0.5245, 0.4755]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[-0.0508,  0.2776],\n",
      "        [-0.2099,  0.2931],\n",
      "        [-0.2447,  0.2724],\n",
      "        [ 0.1531,  0.2109]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4186, 0.5814],\n",
      "        [0.3768, 0.6232],\n",
      "        [0.3735, 0.6265],\n",
      "        [0.4856, 0.5144]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "120\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[0.5831, 0.2091],\n",
      "        [0.3825, 0.1104],\n",
      "        [0.1729, 0.1592],\n",
      "        [0.2060, 0.3157]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5924, 0.4076],\n",
      "        [0.5676, 0.4324],\n",
      "        [0.5034, 0.4966],\n",
      "        [0.4726, 0.5274]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.0514,  0.1766],\n",
      "        [-0.0114,  0.1314],\n",
      "        [-0.0224,  0.1773],\n",
      "        [ 0.0621,  0.2964]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4687, 0.5313],\n",
      "        [0.4643, 0.5357],\n",
      "        [0.4502, 0.5498],\n",
      "        [0.4417, 0.5583]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[-0.2043,  0.0079],\n",
      "        [ 0.2016,  0.3617],\n",
      "        [ 0.0940,  0.0602],\n",
      "        [ 0.0205,  0.1486]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4472, 0.5528],\n",
      "        [0.4601, 0.5399],\n",
      "        [0.5084, 0.4916],\n",
      "        [0.4680, 0.5320]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 0.1608, -0.0938],\n",
      "        [ 0.2789,  0.3415],\n",
      "        [ 0.2819,  0.2944],\n",
      "        [ 0.0010,  0.3656]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5633, 0.4367],\n",
      "        [0.4844, 0.5156],\n",
      "        [0.4969, 0.5031],\n",
      "        [0.4098, 0.5902]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.3123, -0.3419],\n",
      "        [ 0.3095, -0.1920],\n",
      "        [ 0.2921, -0.0462],\n",
      "        [ 0.0573, -0.2401]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6579, 0.3421],\n",
      "        [0.6228, 0.3772],\n",
      "        [0.5838, 0.4162],\n",
      "        [0.5738, 0.4262]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[0.0227, 0.2482],\n",
      "        [0.2370, 0.2610],\n",
      "        [0.2464, 0.3033],\n",
      "        [0.0318, 0.3913]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4439, 0.5561],\n",
      "        [0.4940, 0.5060],\n",
      "        [0.4858, 0.5142],\n",
      "        [0.4111, 0.5889]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.1901,  0.4121],\n",
      "        [ 0.2988,  0.4712],\n",
      "        [ 0.1589,  0.4265],\n",
      "        [-0.0030, -0.0530]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4447, 0.5553],\n",
      "        [0.4570, 0.5430],\n",
      "        [0.4335, 0.5665],\n",
      "        [0.5125, 0.4875]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0623,  0.4348],\n",
      "        [-0.0476,  0.0817],\n",
      "        [ 0.1728,  0.0295],\n",
      "        [ 0.2971,  0.3967]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3782, 0.6218],\n",
      "        [0.4677, 0.5323],\n",
      "        [0.5358, 0.4642],\n",
      "        [0.4751, 0.5249]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.0341,  0.4510],\n",
      "        [-0.2112,  0.2474],\n",
      "        [ 0.0960,  0.3679],\n",
      "        [ 0.0103,  0.2784]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3973, 0.6027],\n",
      "        [0.3873, 0.6127],\n",
      "        [0.4324, 0.5676],\n",
      "        [0.4334, 0.5666]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.4816,  0.7212],\n",
      "        [-0.1701,  0.6416],\n",
      "        [ 0.0453,  0.3209],\n",
      "        [-0.4299,  0.4945]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2310, 0.7690],\n",
      "        [0.3075, 0.6925],\n",
      "        [0.4315, 0.5685],\n",
      "        [0.2841, 0.7159]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "130\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[-0.0269,  0.4118],\n",
      "        [ 0.4174,  0.3622],\n",
      "        [ 0.4074,  0.3613],\n",
      "        [-0.0942,  0.5116]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3921, 0.6079],\n",
      "        [0.5138, 0.4862],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.3530, 0.6470]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[-0.1026,  0.2757],\n",
      "        [-0.0242,  0.3456],\n",
      "        [ 0.1027,  0.1656],\n",
      "        [-0.1991,  0.3120]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4065, 0.5935],\n",
      "        [0.4086, 0.5914],\n",
      "        [0.4843, 0.5157],\n",
      "        [0.3749, 0.6251]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.2212,  0.3607],\n",
      "        [ 0.0832,  0.4355],\n",
      "        [-0.1462,  0.0499],\n",
      "        [-0.2029,  0.4466]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3585, 0.6415],\n",
      "        [0.4128, 0.5872],\n",
      "        [0.4511, 0.5489],\n",
      "        [0.3431, 0.6569]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0291,  0.3723],\n",
      "        [ 0.1696, -0.0313],\n",
      "        [ 0.1968, -0.1224],\n",
      "        [ 0.2373, -0.0033]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4010, 0.5990],\n",
      "        [0.5501, 0.4499],\n",
      "        [0.5791, 0.4209],\n",
      "        [0.5598, 0.4402]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[-0.0690,  0.1576],\n",
      "        [ 0.1411, -0.1856],\n",
      "        [-0.2136,  0.0336],\n",
      "        [-0.2401,  0.3739]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4436, 0.5564],\n",
      "        [0.5810, 0.4190],\n",
      "        [0.4385, 0.5615],\n",
      "        [0.3511, 0.6489]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 0.1941, -0.1095],\n",
      "        [-0.1806, -0.1696],\n",
      "        [-0.1050,  0.1436],\n",
      "        [-0.2596,  0.5821]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5753, 0.4247],\n",
      "        [0.4972, 0.5028],\n",
      "        [0.4382, 0.5618],\n",
      "        [0.3012, 0.6988]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.4069,  0.3293],\n",
      "        [-0.0401, -0.0685],\n",
      "        [ 0.3149, -0.1140],\n",
      "        [ 0.2128, -0.1561]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5194, 0.4806],\n",
      "        [0.5071, 0.4929],\n",
      "        [0.6056, 0.3944],\n",
      "        [0.5912, 0.4088]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.3249,  0.2808],\n",
      "        [ 0.2528,  0.0657],\n",
      "        [ 0.1752, -0.0214],\n",
      "        [-0.1459, -0.0595]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5110, 0.4890],\n",
      "        [0.5467, 0.4533],\n",
      "        [0.5490, 0.4510],\n",
      "        [0.4784, 0.5216]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.2257, -0.1988],\n",
      "        [ 0.5139, -0.1012],\n",
      "        [ 0.4811, -0.0808],\n",
      "        [ 0.0249, -0.2594]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6046, 0.3954],\n",
      "        [0.6491, 0.3509],\n",
      "        [0.6369, 0.3631],\n",
      "        [0.5706, 0.4294]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 1], device='cuda:0') tensor([[-0.5046,  0.2765],\n",
      "        [-0.1138,  0.0062],\n",
      "        [ 0.0677,  0.0873],\n",
      "        [-0.1946,  0.1094]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3141, 0.6859],\n",
      "        [0.4700, 0.5300],\n",
      "        [0.4951, 0.5049],\n",
      "        [0.4246, 0.5754]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "140\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-0.1415, -0.0639],\n",
      "        [ 0.3562, -0.2775],\n",
      "        [ 0.0925, -0.2462],\n",
      "        [ 0.3759, -0.1789]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4806, 0.5194],\n",
      "        [0.6533, 0.3467],\n",
      "        [0.5839, 0.4161],\n",
      "        [0.6353, 0.3647]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.2389, -0.1673],\n",
      "        [ 0.0303, -0.1960],\n",
      "        [ 0.3984, -0.4632],\n",
      "        [ 0.4060, -0.2195]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6002, 0.3998],\n",
      "        [0.5563, 0.4437],\n",
      "        [0.7030, 0.2970],\n",
      "        [0.6514, 0.3486]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.1643, -0.3233],\n",
      "        [ 0.1817,  0.1224],\n",
      "        [ 0.2490,  0.0978],\n",
      "        [ 0.0939, -0.1291]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6196, 0.3804],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.5377, 0.4623],\n",
      "        [0.5555, 0.4445]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[ 0.1566,  0.0220],\n",
      "        [ 0.5727, -0.0648],\n",
      "        [ 0.5084, -0.2517],\n",
      "        [-0.1392, -0.1082]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5336, 0.4664],\n",
      "        [0.6542, 0.3458],\n",
      "        [0.6814, 0.3186],\n",
      "        [0.4922, 0.5078]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[ 0.0410, -0.1241],\n",
      "        [ 0.1459, -0.3573],\n",
      "        [ 0.2577, -0.2237],\n",
      "        [ 0.4110, -0.2071]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5412, 0.4588],\n",
      "        [0.6232, 0.3768],\n",
      "        [0.6181, 0.3819],\n",
      "        [0.6498, 0.3502]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.6848, -0.3563],\n",
      "        [ 0.5523, -0.4953],\n",
      "        [ 0.1887, -0.2808],\n",
      "        [ 0.4461,  0.2378]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7391, 0.2609],\n",
      "        [0.7403, 0.2597],\n",
      "        [0.6153, 0.3847],\n",
      "        [0.5519, 0.4481]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 0, 1], device='cuda:0') tensor([[-0.4079,  0.2094],\n",
      "        [-0.3813,  0.4025],\n",
      "        [-0.2591,  0.1723],\n",
      "        [ 0.1573, -0.0917]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3504, 0.6496],\n",
      "        [0.3135, 0.6865],\n",
      "        [0.3938, 0.6062],\n",
      "        [0.5619, 0.4381]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[ 0.2745, -0.0015],\n",
      "        [ 0.4432, -0.4969],\n",
      "        [ 0.5058, -0.2522],\n",
      "        [ 0.2582, -0.3948]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5686, 0.4314],\n",
      "        [0.7191, 0.2809],\n",
      "        [0.6809, 0.3191],\n",
      "        [0.6577, 0.3423]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.4611, -0.0933],\n",
      "        [ 0.4034, -0.0823],\n",
      "        [ 0.5071, -0.1270],\n",
      "        [ 0.3997, -0.3247]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6352, 0.3648],\n",
      "        [0.6191, 0.3809],\n",
      "        [0.6534, 0.3466],\n",
      "        [0.6736, 0.3264]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[-0.0119,  0.0415],\n",
      "        [ 0.0609, -0.2191],\n",
      "        [ 0.1121, -0.0665],\n",
      "        [ 0.2883,  0.0597]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4867, 0.5133],\n",
      "        [0.5695, 0.4305],\n",
      "        [0.5445, 0.4555],\n",
      "        [0.5569, 0.4431]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "150\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.0645,  0.2155],\n",
      "        [ 0.2408, -0.0603],\n",
      "        [ 0.0700, -0.0305],\n",
      "        [ 0.0363, -0.2601]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4304, 0.5696],\n",
      "        [0.5747, 0.4253],\n",
      "        [0.5251, 0.4749],\n",
      "        [0.5736, 0.4264]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.0423, -0.3365],\n",
      "        [ 0.2079, -0.1260],\n",
      "        [ 0.0883, -0.2033],\n",
      "        [ 0.1720, -0.0888]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5936, 0.4064],\n",
      "        [0.5827, 0.4173],\n",
      "        [0.5724, 0.4276],\n",
      "        [0.5648, 0.4352]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.0758,  0.2333],\n",
      "        [ 0.4329, -0.0487],\n",
      "        [ 0.4498, -0.0997],\n",
      "        [ 0.3633,  0.0922]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4607, 0.5393],\n",
      "        [0.6181, 0.3819],\n",
      "        [0.6340, 0.3660],\n",
      "        [0.5674, 0.4326]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0201, -0.0547],\n",
      "        [ 0.4990, -0.0027],\n",
      "        [ 0.2773, -0.3814],\n",
      "        [ 0.3565, -0.0257]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5086, 0.4914],\n",
      "        [0.6229, 0.3771],\n",
      "        [0.6590, 0.3410],\n",
      "        [0.5944, 0.4056]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0393, -0.3933],\n",
      "        [ 0.2224,  0.0156],\n",
      "        [ 0.1878, -0.1472],\n",
      "        [-0.1716, -0.2720]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5876, 0.4124],\n",
      "        [0.5515, 0.4485],\n",
      "        [0.5830, 0.4170],\n",
      "        [0.5251, 0.4749]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.0479,  0.2043],\n",
      "        [ 0.0421,  0.3354],\n",
      "        [-0.0329,  0.0963],\n",
      "        [-0.2326, -0.1388]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4373, 0.5627],\n",
      "        [0.4272, 0.5728],\n",
      "        [0.4677, 0.5323],\n",
      "        [0.4766, 0.5234]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.0950, -0.1237],\n",
      "        [ 0.1047, -0.2405],\n",
      "        [-0.0068, -0.1195],\n",
      "        [ 0.2182, -0.2628]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5072, 0.4928],\n",
      "        [0.5855, 0.4145],\n",
      "        [0.5281, 0.4719],\n",
      "        [0.6180, 0.3820]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.1526, -0.0920],\n",
      "        [ 0.2020, -0.3282],\n",
      "        [ 0.0497, -0.0389],\n",
      "        [-0.1604, -0.2099]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5609, 0.4391],\n",
      "        [0.6295, 0.3705],\n",
      "        [0.5221, 0.4779],\n",
      "        [0.5124, 0.4876]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 1, 1], device='cuda:0') tensor([[-0.3049,  0.6577],\n",
      "        [-0.6440,  0.1398],\n",
      "        [-0.0298,  0.2296],\n",
      "        [-0.6264,  0.4678]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2764, 0.7236],\n",
      "        [0.3135, 0.6865],\n",
      "        [0.4355, 0.5645],\n",
      "        [0.2508, 0.7492]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.2557, -0.3248],\n",
      "        [-0.0102,  0.1073],\n",
      "        [-0.0747,  0.0451],\n",
      "        [ 0.7756, -0.5804]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6412, 0.3588],\n",
      "        [0.4707, 0.5293],\n",
      "        [0.4701, 0.5299],\n",
      "        [0.7951, 0.2049]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "160\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[-0.3546,  0.4056],\n",
      "        [-0.4706,  0.4254],\n",
      "        [-0.9333,  0.7559],\n",
      "        [ 0.0791,  0.2379]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3186, 0.6814],\n",
      "        [0.2899, 0.7101],\n",
      "        [0.1559, 0.8441],\n",
      "        [0.4604, 0.5396]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 0], device='cuda:0') tensor([[-0.6138,  0.5037],\n",
      "        [-0.1090,  0.1435],\n",
      "        [-0.2435,  0.4679],\n",
      "        [-0.1214, -0.0244]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2465, 0.7535],\n",
      "        [0.4372, 0.5628],\n",
      "        [0.3293, 0.6707],\n",
      "        [0.4758, 0.5242]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[-0.1525,  0.1169],\n",
      "        [ 0.0826,  0.0034],\n",
      "        [-0.5292,  0.6356],\n",
      "        [-0.3758,  0.3835]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4330, 0.5670],\n",
      "        [0.5198, 0.4802],\n",
      "        [0.2378, 0.7622],\n",
      "        [0.3188, 0.6812]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[-0.2338,  0.6038],\n",
      "        [ 0.2010,  0.1673],\n",
      "        [ 0.2518, -0.0738],\n",
      "        [ 0.0469,  0.2328]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3021, 0.6979],\n",
      "        [0.5084, 0.4916],\n",
      "        [0.5807, 0.4193],\n",
      "        [0.4537, 0.5463]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 0], device='cuda:0') tensor([[ 0.3081, -0.0739],\n",
      "        [ 0.0678, -0.1211],\n",
      "        [ 0.0050,  0.1236],\n",
      "        [-0.0398, -0.2809]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5943, 0.4057],\n",
      "        [0.5471, 0.4529],\n",
      "        [0.4704, 0.5296],\n",
      "        [0.5600, 0.4400]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[ 0.4488, -0.2712],\n",
      "        [ 0.4998, -0.1948],\n",
      "        [ 0.4048, -0.2690],\n",
      "        [ 0.6272,  0.0863]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6726, 0.3274],\n",
      "        [0.6670, 0.3330],\n",
      "        [0.6624, 0.3376],\n",
      "        [0.6320, 0.3680]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[ 0.6864, -0.3222],\n",
      "        [ 0.2195, -0.3153],\n",
      "        [ 0.4414, -0.5782],\n",
      "        [ 0.1565, -0.3201]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7328, 0.2672],\n",
      "        [0.6306, 0.3694],\n",
      "        [0.7349, 0.2651],\n",
      "        [0.6169, 0.3831]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.3187, -0.2735],\n",
      "        [ 0.5323, -0.4864],\n",
      "        [ 0.2866, -0.2985],\n",
      "        [ 0.2690, -0.2888]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6439, 0.3561],\n",
      "        [0.7347, 0.2653],\n",
      "        [0.6422, 0.3578],\n",
      "        [0.6359, 0.3641]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.2298, -0.1319],\n",
      "        [ 0.2927, -0.2239],\n",
      "        [ 0.7872, -0.1923],\n",
      "        [ 0.5420, -0.1909]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5894, 0.4106],\n",
      "        [0.6264, 0.3736],\n",
      "        [0.7270, 0.2730],\n",
      "        [0.6754, 0.3246]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.3661, -0.3833],\n",
      "        [ 0.3946, -0.0674],\n",
      "        [ 0.3927, -0.3426],\n",
      "        [ 0.3068,  0.3853]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6791, 0.3209],\n",
      "        [0.6135, 0.3865],\n",
      "        [0.6760, 0.3240],\n",
      "        [0.4804, 0.5196]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "170\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 1.0430, -0.5096],\n",
      "        [ 0.6498, -0.4388],\n",
      "        [ 0.8451, -0.2890],\n",
      "        [ 0.1453, -0.2070]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8253, 0.1747],\n",
      "        [0.7481, 0.2519],\n",
      "        [0.7566, 0.2434],\n",
      "        [0.5872, 0.4128]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.5998, -0.3109],\n",
      "        [ 0.3472, -0.3803],\n",
      "        [ 0.4208, -0.4282],\n",
      "        [ 0.2683, -0.5036]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7132, 0.2868],\n",
      "        [0.6742, 0.3258],\n",
      "        [0.7003, 0.2997],\n",
      "        [0.6839, 0.3161]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[ 0.6073,  0.0511],\n",
      "        [ 0.5325,  0.0396],\n",
      "        [ 0.2331, -0.2758],\n",
      "        [ 0.7352, -0.1793]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6356, 0.3644],\n",
      "        [0.6208, 0.3792],\n",
      "        [0.6245, 0.3755],\n",
      "        [0.7139, 0.2861]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[ 0.2572, -0.2190],\n",
      "        [ 0.4205, -0.1584],\n",
      "        [ 0.2074, -0.3172],\n",
      "        [ 0.2375, -0.1822]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6169, 0.3831],\n",
      "        [0.6408, 0.3592],\n",
      "        [0.6282, 0.3718],\n",
      "        [0.6034, 0.3966]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.4023,  0.0217],\n",
      "        [ 0.3321, -0.3866],\n",
      "        [ 0.4621, -0.2807],\n",
      "        [ 0.2104, -0.2871]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5940, 0.4060],\n",
      "        [0.6723, 0.3277],\n",
      "        [0.6776, 0.3224],\n",
      "        [0.6219, 0.3781]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[ 0.1585,  0.1460],\n",
      "        [ 0.2600, -0.2228],\n",
      "        [-0.0150, -0.0660],\n",
      "        [ 0.2846, -0.5970]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5031, 0.4969],\n",
      "        [0.6184, 0.3816],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.7072, 0.2928]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.4594, -0.2166],\n",
      "        [ 0.0587,  0.1550],\n",
      "        [ 0.3706, -0.2064],\n",
      "        [ 0.0157,  0.0294]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6629, 0.3371],\n",
      "        [0.4759, 0.5241],\n",
      "        [0.6404, 0.3596],\n",
      "        [0.4966, 0.5034]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.5236, -0.0990],\n",
      "        [ 0.3885, -0.4022],\n",
      "        [ 0.4843, -0.5086],\n",
      "        [ 0.3363, -0.3891]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6508, 0.3492],\n",
      "        [0.6880, 0.3120],\n",
      "        [0.7297, 0.2703],\n",
      "        [0.6738, 0.3262]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[ 0.2899, -0.1250],\n",
      "        [ 0.4054, -0.5275],\n",
      "        [ 0.3425, -0.3832],\n",
      "        [ 0.3277, -0.2477]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6023, 0.3977],\n",
      "        [0.7177, 0.2823],\n",
      "        [0.6739, 0.3261],\n",
      "        [0.6400, 0.3600]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 1], device='cuda:0') tensor([[ 0.1635, -0.0882],\n",
      "        [ 0.4761, -0.2578],\n",
      "        [ 0.5316, -0.1962],\n",
      "        [ 0.0538, -0.1139]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5626, 0.4374],\n",
      "        [0.6757, 0.3243],\n",
      "        [0.6743, 0.3257],\n",
      "        [0.5418, 0.4582]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "180\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-0.1675,  0.2783],\n",
      "        [ 0.2923,  0.0038],\n",
      "        [-0.0411, -0.2522],\n",
      "        [ 0.1361, -0.0128]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3904, 0.6096],\n",
      "        [0.5716, 0.4284],\n",
      "        [0.5526, 0.4474],\n",
      "        [0.5371, 0.4629]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 0.3711, -0.3656],\n",
      "        [ 0.2161, -0.0392],\n",
      "        [ 0.5306, -0.1646],\n",
      "        [ 0.4601,  0.0887]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6763, 0.3237],\n",
      "        [0.5635, 0.4365],\n",
      "        [0.6671, 0.3329],\n",
      "        [0.5918, 0.4082]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[ 0.1335,  0.3721],\n",
      "        [ 0.1146, -0.2934],\n",
      "        [ 0.1488,  0.0444],\n",
      "        [ 0.6220, -0.5777]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4406, 0.5594],\n",
      "        [0.6006, 0.3994],\n",
      "        [0.5261, 0.4739],\n",
      "        [0.7685, 0.2315]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.3001, -0.2703],\n",
      "        [ 0.3377, -0.3210],\n",
      "        [ 0.1765, -0.3818],\n",
      "        [ 0.3178, -0.5039]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6389, 0.3611],\n",
      "        [0.6590, 0.3410],\n",
      "        [0.6361, 0.3639],\n",
      "        [0.6946, 0.3054]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.2177, -0.3216],\n",
      "        [-0.3732,  0.3229],\n",
      "        [ 0.0749,  0.1842],\n",
      "        [-0.0806,  0.3584]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6317, 0.3683],\n",
      "        [0.3327, 0.6673],\n",
      "        [0.4727, 0.5273],\n",
      "        [0.3920, 0.6080]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.0253, -0.3040],\n",
      "        [ 0.4236, -0.1741],\n",
      "        [ 0.1124,  0.0096],\n",
      "        [ 0.2906, -0.4077]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5816, 0.4184],\n",
      "        [0.6451, 0.3549],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.6678, 0.3322]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[-0.0553, -0.0667],\n",
      "        [ 0.2640,  0.0751],\n",
      "        [-0.1011,  0.5583],\n",
      "        [-0.0128,  0.4534]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5029, 0.4971],\n",
      "        [0.5471, 0.4529],\n",
      "        [0.3409, 0.6591],\n",
      "        [0.3855, 0.6145]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.0351,  0.4489],\n",
      "        [ 0.1021,  0.2933],\n",
      "        [-0.0039,  0.2714],\n",
      "        [-0.0201,  0.5297]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3813, 0.6187],\n",
      "        [0.4523, 0.5477],\n",
      "        [0.4316, 0.5684],\n",
      "        [0.3659, 0.6341]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.0848, -0.2990],\n",
      "        [-0.0648,  0.7958],\n",
      "        [-0.3972,  0.7029],\n",
      "        [ 0.0540,  0.2841]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5948, 0.4052],\n",
      "        [0.2972, 0.7028],\n",
      "        [0.2497, 0.7503],\n",
      "        [0.4427, 0.5573]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.3480, -0.4200],\n",
      "        [ 0.1762,  0.2961],\n",
      "        [ 0.3260, -0.1730],\n",
      "        [-0.0182,  0.1486]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6831, 0.3169],\n",
      "        [0.4701, 0.5299],\n",
      "        [0.6222, 0.3778],\n",
      "        [0.4584, 0.5416]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "190\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 0], device='cuda:0') tensor([[ 0.4283, -0.1764],\n",
      "        [ 0.3122, -0.2452],\n",
      "        [ 0.1348,  0.2281],\n",
      "        [ 0.0372, -0.2904]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6467, 0.3533],\n",
      "        [0.6358, 0.3642],\n",
      "        [0.4767, 0.5233],\n",
      "        [0.5812, 0.4188]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 0.1161, -0.0238],\n",
      "        [ 0.2399,  0.1745],\n",
      "        [-0.3545,  0.6555],\n",
      "        [-0.3099,  0.6993]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5349, 0.4651],\n",
      "        [0.5163, 0.4837],\n",
      "        [0.2670, 0.7330],\n",
      "        [0.2671, 0.7329]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[ 0.1019, -0.0947],\n",
      "        [ 0.0747, -0.1499],\n",
      "        [ 0.2895, -0.0104],\n",
      "        [ 0.5836, -0.4993]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5490, 0.4510],\n",
      "        [0.5559, 0.4441],\n",
      "        [0.5744, 0.4256],\n",
      "        [0.7470, 0.2530]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-0.4767,  0.8442],\n",
      "        [-0.5203,  0.8972],\n",
      "        [-0.1733,  0.7071],\n",
      "        [ 0.3722, -0.3405]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2107, 0.7893],\n",
      "        [0.1951, 0.8049],\n",
      "        [0.2931, 0.7069],\n",
      "        [0.6710, 0.3290]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[-0.4132,  0.5171],\n",
      "        [-0.5056,  1.2058],\n",
      "        [ 0.0671,  0.2614],\n",
      "        [-0.9674,  1.6121]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2829, 0.7171],\n",
      "        [0.1530, 0.8470],\n",
      "        [0.4516, 0.5484],\n",
      "        [0.0705, 0.9295]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[-0.8896,  1.6740],\n",
      "        [-0.3002,  0.6782],\n",
      "        [-0.1812,  0.4998],\n",
      "        [-0.2017,  0.7952]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0715, 0.9285],\n",
      "        [0.2732, 0.7268],\n",
      "        [0.3360, 0.6640],\n",
      "        [0.2695, 0.7305]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.0210, -0.3238],\n",
      "        [-0.4001,  0.6224],\n",
      "        [ 0.2597, -0.5886],\n",
      "        [ 0.4470, -0.1941]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5854, 0.4146],\n",
      "        [0.2645, 0.7355],\n",
      "        [0.7002, 0.2998],\n",
      "        [0.6550, 0.3450]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.8564, -0.9656],\n",
      "        [-0.6712,  1.3021],\n",
      "        [ 0.0943,  0.1404],\n",
      "        [-0.1096,  0.0328]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8608, 0.1392],\n",
      "        [0.1220, 0.8780],\n",
      "        [0.4885, 0.5115],\n",
      "        [0.4645, 0.5355]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 1, 1], device='cuda:0') tensor([[-0.6735,  1.2591],\n",
      "        [ 0.2440, -0.5906],\n",
      "        [-0.3437,  1.2740],\n",
      "        [-0.5668,  0.9196]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1265, 0.8735],\n",
      "        [0.6973, 0.3027],\n",
      "        [0.1655, 0.8345],\n",
      "        [0.1845, 0.8155]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[-0.0042,  0.1425],\n",
      "        [-0.2217,  0.4099],\n",
      "        [ 0.4304, -0.8389],\n",
      "        [ 0.5815, -0.5253]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4634, 0.5366],\n",
      "        [0.3471, 0.6529],\n",
      "        [0.7806, 0.2194],\n",
      "        [0.7515, 0.2485]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "200\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.2072,  0.0996],\n",
      "        [ 0.5356, -0.5181],\n",
      "        [-0.7852,  1.2063],\n",
      "        [-0.6288,  1.3031]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5269, 0.4731],\n",
      "        [0.7415, 0.2585],\n",
      "        [0.1201, 0.8799],\n",
      "        [0.1265, 0.8735]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[-0.1134,  0.4670],\n",
      "        [ 0.0730,  0.4152],\n",
      "        [ 0.4851,  0.0803],\n",
      "        [-0.6385,  1.2615]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3588, 0.6412],\n",
      "        [0.4153, 0.5847],\n",
      "        [0.5998, 0.4002],\n",
      "        [0.1301, 0.8699]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.5195,  0.9176],\n",
      "        [-1.3251,  1.4596],\n",
      "        [-0.5486,  1.3829],\n",
      "        [-1.0936,  1.8496]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1920, 0.8080],\n",
      "        [0.0582, 0.9418],\n",
      "        [0.1266, 0.8734],\n",
      "        [0.0501, 0.9499]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.7505, -0.9168],\n",
      "        [ 0.8484, -0.9765],\n",
      "        [ 0.5589, -0.5671],\n",
      "        [ 0.0893, -0.2825]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8412, 0.1588],\n",
      "        [0.8611, 0.1389],\n",
      "        [0.7551, 0.2449],\n",
      "        [0.5919, 0.4081]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 1, 1], device='cuda:0') tensor([[ 0.1296, -0.0060],\n",
      "        [ 0.9496, -0.6369],\n",
      "        [ 0.3414, -0.0872],\n",
      "        [-0.1411,  0.4340]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5338, 0.4662],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.6055, 0.3945],\n",
      "        [0.3600, 0.6400]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 0, 0], device='cuda:0') tensor([[ 0.0766, -0.0815],\n",
      "        [ 0.3025,  0.2792],\n",
      "        [ 0.9123, -0.7335],\n",
      "        [-0.2616,  1.0539]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5394, 0.4606],\n",
      "        [0.5058, 0.4942],\n",
      "        [0.8383, 0.1617],\n",
      "        [0.2116, 0.7884]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.5448, -0.0382],\n",
      "        [-0.6076,  1.1055],\n",
      "        [ 0.1219, -0.3868],\n",
      "        [-0.6149,  1.5002]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3760, 0.6240],\n",
      "        [0.1528, 0.8472],\n",
      "        [0.6245, 0.3755],\n",
      "        [0.1076, 0.8924]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.9668,  1.2112],\n",
      "        [ 0.2918,  0.1539],\n",
      "        [-0.5868,  0.8525],\n",
      "        [-1.1741,  1.8671]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1017, 0.8983],\n",
      "        [0.5344, 0.4656],\n",
      "        [0.1917, 0.8083],\n",
      "        [0.0456, 0.9544]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.5495, -0.8270],\n",
      "        [ 0.0650,  0.2818],\n",
      "        [ 0.6321, -0.9302],\n",
      "        [ 0.7334, -1.2353]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7984, 0.2016],\n",
      "        [0.4460, 0.5540],\n",
      "        [0.8267, 0.1733],\n",
      "        [0.8775, 0.1225]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 1, 1], device='cuda:0') tensor([[-1.7120,  2.2745],\n",
      "        [-0.8549,  1.3772],\n",
      "        [-0.8462,  1.1779],\n",
      "        [-0.2364,  1.0770]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0182, 0.9818],\n",
      "        [0.0969, 0.9031],\n",
      "        [0.1167, 0.8833],\n",
      "        [0.2119, 0.7881]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "210\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.8979, -0.9938],\n",
      "        [ 0.4578, -0.6118],\n",
      "        [-0.4607,  1.2781],\n",
      "        [-0.2006,  0.6208]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8690, 0.1310],\n",
      "        [0.7445, 0.2555],\n",
      "        [0.1495, 0.8505],\n",
      "        [0.3055, 0.6945]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[-1.0398,  1.7177],\n",
      "        [-1.0014,  1.6288],\n",
      "        [-0.8419,  1.4461],\n",
      "        [ 0.7059, -1.5697]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0597, 0.9403],\n",
      "        [0.0672, 0.9328],\n",
      "        [0.0921, 0.9079],\n",
      "        [0.9068, 0.0932]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-1.3311,  1.4398],\n",
      "        [-0.6101,  1.7187],\n",
      "        [-0.8038,  1.5435],\n",
      "        [-0.2250,  1.1213]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0589, 0.9411],\n",
      "        [0.0888, 0.9112],\n",
      "        [0.0873, 0.9127],\n",
      "        [0.2065, 0.7935]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.4521, -0.8077],\n",
      "        [ 0.1837, -0.6408],\n",
      "        [-0.1684,  0.1247],\n",
      "        [ 0.5439, -0.7318]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7790, 0.2210],\n",
      "        [0.6952, 0.3048],\n",
      "        [0.4273, 0.5727],\n",
      "        [0.7817, 0.2183]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-1.0300,  1.5773],\n",
      "        [-1.3882,  1.8125],\n",
      "        [-0.6579,  1.5749],\n",
      "        [-1.3362,  2.1919]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0687, 0.9313],\n",
      "        [0.0391, 0.9609],\n",
      "        [0.0968, 0.9032],\n",
      "        [0.0285, 0.9715]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[-0.6897,  1.5104],\n",
      "        [ 0.7301, -0.9005],\n",
      "        [-0.2268,  0.5506],\n",
      "        [ 0.0559, -0.1217]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0997, 0.9003],\n",
      "        [0.8363, 0.1637],\n",
      "        [0.3149, 0.6851],\n",
      "        [0.5443, 0.4557]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 0.5924, -0.6181],\n",
      "        [ 0.3682, -0.3908],\n",
      "        [-1.8059,  2.2173],\n",
      "        [-0.3081,  0.4934]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7704, 0.2296],\n",
      "        [0.6811, 0.3189],\n",
      "        [0.0176, 0.9824],\n",
      "        [0.3097, 0.6903]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-1.5403,  2.1100],\n",
      "        [-1.2117,  2.4987],\n",
      "        [ 1.1224, -1.3539],\n",
      "        [ 0.7675, -1.2309]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0253, 0.9747],\n",
      "        [0.0239, 0.9761],\n",
      "        [0.9225, 0.0775],\n",
      "        [0.8806, 0.1194]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-0.7879,  1.2909],\n",
      "        [ 0.6002, -0.7872],\n",
      "        [-1.4398,  2.3625],\n",
      "        [-1.8147,  2.5454]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1112, 0.8888],\n",
      "        [0.8002, 0.1998],\n",
      "        [0.0218, 0.9782],\n",
      "        [0.0126, 0.9874]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.6059, -0.7407],\n",
      "        [-0.9470,  1.6161],\n",
      "        [ 1.3085, -1.8317],\n",
      "        [ 0.3596, -0.5246]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7936, 0.2064],\n",
      "        [0.0715, 0.9285],\n",
      "        [0.9585, 0.0415],\n",
      "        [0.7077, 0.2923]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "220\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 1.5286, -1.4915],\n",
      "        [-2.0002,  2.1694],\n",
      "        [ 0.8548, -0.9568],\n",
      "        [-1.1004,  1.6494]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9535, 0.0465],\n",
      "        [0.0152, 0.9848],\n",
      "        [0.8596, 0.1404],\n",
      "        [0.0601, 0.9399]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 0], device='cuda:0') tensor([[ 0.9608, -1.1775],\n",
      "        [-0.8745,  1.3983],\n",
      "        [ 0.6938, -0.5765],\n",
      "        [ 1.0322, -0.8025]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8946, 0.1054],\n",
      "        [0.0934, 0.9066],\n",
      "        [0.7808, 0.2192],\n",
      "        [0.8623, 0.1377]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[-1.4997,  1.9307],\n",
      "        [-1.1763,  2.1473],\n",
      "        [ 1.3081, -1.0175],\n",
      "        [ 1.2558, -1.3256]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0314, 0.9686],\n",
      "        [0.0348, 0.9652],\n",
      "        [0.9110, 0.0890],\n",
      "        [0.9297, 0.0703]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[-0.6868,  1.2953],\n",
      "        [-0.7402,  1.6900],\n",
      "        [-1.2828,  2.0974],\n",
      "        [ 1.1308, -1.2937]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1211, 0.8789],\n",
      "        [0.0809, 0.9191],\n",
      "        [0.0329, 0.9671],\n",
      "        [0.9187, 0.0813]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.7252, -1.9085],\n",
      "        [ 1.2021, -1.7607],\n",
      "        [ 1.1788, -1.6187],\n",
      "        [ 1.6938, -1.9919]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9743, 0.0257],\n",
      "        [0.9509, 0.0491],\n",
      "        [0.9425, 0.0575],\n",
      "        [0.9755, 0.0245]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.1322, -1.2059],\n",
      "        [ 1.5971, -2.1105],\n",
      "        [ 1.4020, -1.9159],\n",
      "        [ 1.1492, -0.6182]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9120, 0.0880],\n",
      "        [0.9761, 0.0239],\n",
      "        [0.9650, 0.0350],\n",
      "        [0.8541, 0.1459]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.1666, -0.3897],\n",
      "        [ 1.4131, -1.7881],\n",
      "        [ 0.7885, -1.3959],\n",
      "        [ 1.5891, -2.2996]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6356, 0.3644],\n",
      "        [0.9609, 0.0391],\n",
      "        [0.8988, 0.1012],\n",
      "        [0.9799, 0.0201]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.1786, -2.0434],\n",
      "        [ 1.0563, -2.0387],\n",
      "        [-1.0714,  1.5689],\n",
      "        [ 0.9672, -1.3905]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9855, 0.0145],\n",
      "        [0.9567, 0.0433],\n",
      "        [0.0666, 0.9334],\n",
      "        [0.9135, 0.0865]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 0, 0], device='cuda:0') tensor([[ 1.5424, -1.7049],\n",
      "        [-1.8325,  2.7264],\n",
      "        [ 0.8897, -0.5412],\n",
      "        [ 1.4001, -1.6852]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9626, 0.0374],\n",
      "        [0.0104, 0.9896],\n",
      "        [0.8070, 0.1930],\n",
      "        [0.9563, 0.0437]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 0.6745, -0.9301],\n",
      "        [-1.7450,  2.7157],\n",
      "        [-0.6751,  1.0222],\n",
      "        [-0.6830,  1.3481]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8327, 0.1673],\n",
      "        [0.0114, 0.9886],\n",
      "        [0.1548, 0.8452],\n",
      "        [0.1160, 0.8840]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "230\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[ 0.2067, -0.3154],\n",
      "        [-0.2714,  0.6411],\n",
      "        [-0.9897,  1.6959],\n",
      "        [ 0.9845, -1.5149]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6276, 0.3724],\n",
      "        [0.2865, 0.7135],\n",
      "        [0.0638, 0.9362],\n",
      "        [0.9241, 0.0759]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.1151, -2.0503],\n",
      "        [ 1.4506, -2.2535],\n",
      "        [ 1.9878, -2.3928],\n",
      "        [ 2.3079, -2.6601]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9847, 0.0153],\n",
      "        [0.9760, 0.0240],\n",
      "        [0.9876, 0.0124],\n",
      "        [0.9931, 0.0069]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 1.7324, -1.8915],\n",
      "        [ 1.5008, -1.9423],\n",
      "        [-1.9590,  2.7095],\n",
      "        [-1.7486,  2.5783]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9740, 0.0260],\n",
      "        [0.9690, 0.0310],\n",
      "        [0.0093, 0.9907],\n",
      "        [0.0130, 0.9870]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.3855, -2.0203],\n",
      "        [ 2.1249, -2.7544],\n",
      "        [ 1.4157, -1.6776],\n",
      "        [ 0.1277,  0.1703]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9679, 0.0321],\n",
      "        [0.9925, 0.0075],\n",
      "        [0.9566, 0.0434],\n",
      "        [0.4893, 0.5107]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-1.7177,  2.5963],\n",
      "        [-1.8945,  2.6855],\n",
      "        [-1.8383,  2.3314],\n",
      "        [-2.2689,  2.2594]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0132, 0.9868],\n",
      "        [0.0102, 0.9898],\n",
      "        [0.0152, 0.9848],\n",
      "        [0.0107, 0.9893]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 1], device='cuda:0') tensor([[ 0.1666, -0.2163],\n",
      "        [-1.2292,  1.5567],\n",
      "        [ 2.4713, -2.2343],\n",
      "        [-2.0906,  2.7797]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5946, 0.4054],\n",
      "        [0.0581, 0.9419],\n",
      "        [0.9910, 0.0090],\n",
      "        [0.0076, 0.9924]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[-1.6517,  2.5119],\n",
      "        [ 1.4281, -1.3279],\n",
      "        [ 2.0139, -2.7240],\n",
      "        [ 1.6422, -2.2667]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0153, 0.9847],\n",
      "        [0.9403, 0.0597],\n",
      "        [0.9913, 0.0087],\n",
      "        [0.9803, 0.0197]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 0, 0], device='cuda:0') tensor([[-1.9064,  2.4451],\n",
      "        [-2.2881,  2.7401],\n",
      "        [-0.5963,  1.0509],\n",
      "        [-1.5748,  2.2688]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0127, 0.9873],\n",
      "        [0.0065, 0.9935],\n",
      "        [0.1615, 0.8385],\n",
      "        [0.0210, 0.9790]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[ 2.0295, -2.4890],\n",
      "        [ 0.0353,  0.2306],\n",
      "        [-1.5217,  2.1498],\n",
      "        [-1.8307,  2.7509]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9892, 0.0108],\n",
      "        [0.4513, 0.5487],\n",
      "        [0.0248, 0.9752],\n",
      "        [0.0101, 0.9899]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-2.0454,  3.0097],\n",
      "        [-1.8249,  2.7350],\n",
      "        [-1.5014,  2.8068],\n",
      "        [-0.0381, -0.2990]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0063, 0.9937],\n",
      "        [0.0104, 0.9896],\n",
      "        [0.0133, 0.9867],\n",
      "        [0.5649, 0.4351]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "240\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[-2.1792,  2.9531],\n",
      "        [-1.8885,  2.6590],\n",
      "        [-1.9329,  3.0637],\n",
      "        [-1.6660,  1.6840]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0059, 0.9941],\n",
      "        [0.0105, 0.9895],\n",
      "        [0.0067, 0.9933],\n",
      "        [0.0339, 0.9661]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 1.3968, -1.6689],\n",
      "        [-2.4480,  3.2143],\n",
      "        [-0.1097,  0.1550],\n",
      "        [-2.0279,  3.4116]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9555, 0.0445],\n",
      "        [0.0035, 0.9965],\n",
      "        [0.4342, 0.5658],\n",
      "        [0.0043, 0.9957]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[ 1.8676, -2.6972],\n",
      "        [ 2.5271, -2.7401],\n",
      "        [ 2.5286, -2.7875],\n",
      "        [-2.1588,  2.8868]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9897, 0.0103],\n",
      "        [0.9949, 0.0051],\n",
      "        [0.9951, 0.0049],\n",
      "        [0.0064, 0.9936]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-1.9172,  2.4554],\n",
      "        [-2.4770,  2.7836],\n",
      "        [-2.0940,  3.0986],\n",
      "        [-2.0225,  2.8233]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0125, 0.9875],\n",
      "        [0.0052, 0.9948],\n",
      "        [0.0055, 0.9945],\n",
      "        [0.0078, 0.9922]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-2.3295,  2.9739],\n",
      "        [ 1.4859, -2.3145],\n",
      "        [-2.1854,  2.9731],\n",
      "        [-1.8708,  2.2206]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0050, 0.9950],\n",
      "        [0.9781, 0.0219],\n",
      "        [0.0057, 0.9943],\n",
      "        [0.0164, 0.9836]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 1, 1], device='cuda:0') tensor([[-2.1733,  2.3846],\n",
      "        [-1.4004,  1.9327],\n",
      "        [-2.0070,  2.6936],\n",
      "        [-1.7094,  2.4882]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0104, 0.9896],\n",
      "        [0.0345, 0.9655],\n",
      "        [0.0090, 0.9910],\n",
      "        [0.0148, 0.9852]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-2.7422,  3.1952],\n",
      "        [-2.3877,  2.8450],\n",
      "        [ 1.8122, -2.7861],\n",
      "        [-1.5949,  2.2132]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0026, 0.9974],\n",
      "        [0.0053, 0.9947],\n",
      "        [0.9900, 0.0100],\n",
      "        [0.0217, 0.9783]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 1], device='cuda:0') tensor([[-2.1240,  3.0330],\n",
      "        [-2.3707,  2.7487],\n",
      "        [ 0.2939,  0.0394],\n",
      "        [-1.6174,  2.2848]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0057, 0.9943],\n",
      "        [0.0059, 0.9941],\n",
      "        [0.5633, 0.4367],\n",
      "        [0.0198, 0.9802]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[-0.6242,  1.5059],\n",
      "        [ 0.5243, -0.7248],\n",
      "        [ 1.8016, -1.9599],\n",
      "        [-1.6183,  2.6519]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1062, 0.8938],\n",
      "        [0.7771, 0.2229],\n",
      "        [0.9773, 0.0227],\n",
      "        [0.0138, 0.9862]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-1.3225,  1.9556],\n",
      "        [-1.7475,  2.4989],\n",
      "        [-1.8466,  2.7321],\n",
      "        [-1.9273,  2.6486]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0363, 0.9637],\n",
      "        [0.0141, 0.9859],\n",
      "        [0.0102, 0.9898],\n",
      "        [0.0102, 0.9898]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "250\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 0], device='cuda:0') tensor([[ 0.1482,  0.1127],\n",
      "        [-2.1297,  2.7683],\n",
      "        [-2.0376,  2.3959],\n",
      "        [ 0.4828, -0.0767]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5089, 0.4911],\n",
      "        [0.0074, 0.9926],\n",
      "        [0.0117, 0.9883],\n",
      "        [0.6364, 0.3636]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[ 1.0485, -1.4293],\n",
      "        [ 0.9405, -1.5511],\n",
      "        [ 2.3528, -2.9449],\n",
      "        [ 1.5682, -1.7949]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9226, 0.0774],\n",
      "        [0.9236, 0.0764],\n",
      "        [0.9950, 0.0050],\n",
      "        [0.9665, 0.0335]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 0], device='cuda:0') tensor([[-1.9698,  2.6012],\n",
      "        [-2.1166,  2.8974],\n",
      "        [-0.9718,  1.2486],\n",
      "        [ 0.6628, -0.9770]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0102, 0.9898],\n",
      "        [0.0066, 0.9934],\n",
      "        [0.0979, 0.9021],\n",
      "        [0.8375, 0.1625]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-1.0957,  1.6174],\n",
      "        [-1.8167,  2.7871],\n",
      "        [ 2.3265, -2.5539],\n",
      "        [ 2.1827, -2.2808]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0622, 0.9378],\n",
      "        [0.0099, 0.9901],\n",
      "        [0.9925, 0.0075],\n",
      "        [0.9886, 0.0114]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.0661, -2.3916],\n",
      "        [ 2.3526, -3.0049],\n",
      "        [ 2.1988, -2.6417],\n",
      "        [ 2.2651, -3.5543]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9885, 0.0115],\n",
      "        [0.9953, 0.0047],\n",
      "        [0.9922, 0.0078],\n",
      "        [0.9970, 0.0030]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[ 1.8905, -2.4358],\n",
      "        [ 1.2799, -2.0452],\n",
      "        [ 2.5343, -2.7900],\n",
      "        [ 1.1859, -1.3398]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9870, 0.0130],\n",
      "        [0.9653, 0.0347],\n",
      "        [0.9952, 0.0048],\n",
      "        [0.9259, 0.0741]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.3782, -2.6743],\n",
      "        [ 2.4022, -2.7835],\n",
      "        [ 1.9105, -2.6238],\n",
      "        [ 2.1671, -2.8853]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9936, 0.0064],\n",
      "        [0.9944, 0.0056],\n",
      "        [0.9894, 0.0106],\n",
      "        [0.9936, 0.0064]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.6135, -3.1008],\n",
      "        [ 2.6907, -2.6783],\n",
      "        [ 1.6696, -2.6471],\n",
      "        [ 2.5943, -3.7966]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9967, 0.0033],\n",
      "        [0.9954, 0.0046],\n",
      "        [0.9868, 0.0132],\n",
      "        [0.9983, 0.0017]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 2.2164, -2.4768],\n",
      "        [ 0.4704, -0.6577],\n",
      "        [-0.2946,  0.5590],\n",
      "        [ 2.0072, -2.7280]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9909, 0.0091],\n",
      "        [0.7555, 0.2445],\n",
      "        [0.2987, 0.7013],\n",
      "        [0.9913, 0.0087]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 2.3106, -3.2108],\n",
      "        [ 2.5409, -2.9599],\n",
      "        [-1.1407,  1.7771],\n",
      "        [ 0.5619, -0.5248]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9960, 0.0040],\n",
      "        [0.9959, 0.0041],\n",
      "        [0.0513, 0.9487],\n",
      "        [0.7477, 0.2523]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "260\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 0.6769, -0.7933],\n",
      "        [ 1.7426, -2.2556],\n",
      "        [ 2.4005, -3.0389],\n",
      "        [-0.8581,  1.4672]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8131, 0.1869],\n",
      "        [0.9820, 0.0180],\n",
      "        [0.9957, 0.0043],\n",
      "        [0.0891, 0.9109]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-2.0491,  2.6437],\n",
      "        [ 0.0467,  0.8424],\n",
      "        [ 1.5206, -1.6584],\n",
      "        [-0.3091,  0.6540]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0091, 0.9909],\n",
      "        [0.3109, 0.6891],\n",
      "        [0.9600, 0.0400],\n",
      "        [0.2763, 0.7237]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.5099, -2.7969],\n",
      "        [ 2.2716, -2.4590],\n",
      "        [ 1.6585, -2.2158],\n",
      "        [ 1.8587, -2.0241]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9951, 0.0049],\n",
      "        [0.9913, 0.0087],\n",
      "        [0.9797, 0.0203],\n",
      "        [0.9798, 0.0202]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.4130, -1.8099],\n",
      "        [ 2.0741, -2.5796],\n",
      "        [ 1.4782, -2.0139],\n",
      "        [ 2.6868, -3.0127]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9617, 0.0383],\n",
      "        [0.9906, 0.0094],\n",
      "        [0.9705, 0.0295],\n",
      "        [0.9967, 0.0033]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.5660,  1.0378],\n",
      "        [-0.4585,  0.6587],\n",
      "        [ 2.2619, -3.0724],\n",
      "        [ 2.4361, -3.2789]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1674, 0.8326],\n",
      "        [0.2465, 0.7535],\n",
      "        [0.9952, 0.0048],\n",
      "        [0.9967, 0.0033]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-1.6892,  2.0030],\n",
      "        [-0.7714,  1.3859],\n",
      "        [-0.1313,  0.4247],\n",
      "        [-2.1912,  2.2399]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0243, 0.9757],\n",
      "        [0.1037, 0.8963],\n",
      "        [0.3645, 0.6355],\n",
      "        [0.0118, 0.9882]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.3932, -2.9171],\n",
      "        [ 1.7339, -1.5516],\n",
      "        [ 1.6321, -1.8272],\n",
      "        [ 2.2735, -2.4006]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9951, 0.0049],\n",
      "        [0.9639, 0.0361],\n",
      "        [0.9695, 0.0305],\n",
      "        [0.9908, 0.0092]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-1.5936,  2.2282],\n",
      "        [-2.0947,  2.7885],\n",
      "        [ 2.0906, -1.8205],\n",
      "        [ 2.2377, -2.5285]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0214, 0.9786],\n",
      "        [0.0075, 0.9925],\n",
      "        [0.9804, 0.0196],\n",
      "        [0.9916, 0.0084]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[-0.6998,  0.9040],\n",
      "        [ 2.3112, -2.7259],\n",
      "        [-0.0851,  0.6305],\n",
      "        [ 2.2060, -2.4647]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1675, 0.8325],\n",
      "        [0.9935, 0.0065],\n",
      "        [0.3284, 0.6716],\n",
      "        [0.9907, 0.0093]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.5765, -2.4317],\n",
      "        [-0.2847,  0.4016],\n",
      "        [ 2.3314, -2.6812],\n",
      "        [ 1.8708, -2.1621]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9934, 0.0066],\n",
      "        [0.3348, 0.6652],\n",
      "        [0.9934, 0.0066],\n",
      "        [0.9826, 0.0174]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "270\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 1], device='cuda:0') tensor([[-1.1107,  1.8325],\n",
      "        [ 0.8121, -0.5170],\n",
      "        [ 2.2019, -2.5329],\n",
      "        [ 0.1442,  0.3914]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0501, 0.9499],\n",
      "        [0.7907, 0.2093],\n",
      "        [0.9913, 0.0087],\n",
      "        [0.4385, 0.5615]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.4033, -2.8622],\n",
      "        [ 0.5457, -0.5738],\n",
      "        [ 2.2108, -2.3485],\n",
      "        [ 2.2697, -2.7512]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9949, 0.0051],\n",
      "        [0.7539, 0.2461],\n",
      "        [0.9896, 0.0104],\n",
      "        [0.9934, 0.0066]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-1.3180,  1.8617],\n",
      "        [-0.9533,  2.0401],\n",
      "        [-1.2486,  1.6274],\n",
      "        [-1.0315,  1.5178]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0399, 0.9601],\n",
      "        [0.0477, 0.9523],\n",
      "        [0.0534, 0.9466],\n",
      "        [0.0725, 0.9275]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[ 1.8650, -2.2319],\n",
      "        [-1.5968,  3.0169],\n",
      "        [-0.0643,  0.9965],\n",
      "        [ 1.3399, -1.6151]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9836, 0.0164],\n",
      "        [0.0098, 0.9902],\n",
      "        [0.2572, 0.7428],\n",
      "        [0.9505, 0.0495]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.6332, -1.9873],\n",
      "        [ 2.3257, -2.3178],\n",
      "        [ 2.0079, -2.4992],\n",
      "        [ 1.8926, -2.2164]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9739, 0.0261],\n",
      "        [0.9905, 0.0095],\n",
      "        [0.9891, 0.0109],\n",
      "        [0.9838, 0.0162]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-1.2660,  1.5816],\n",
      "        [-0.9135,  0.8068],\n",
      "        [-2.1792,  3.0096],\n",
      "        [ 1.1782, -1.3465]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0548, 0.9452],\n",
      "        [0.1518, 0.8482],\n",
      "        [0.0055, 0.9945],\n",
      "        [0.9259, 0.0741]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.8194, -2.1263],\n",
      "        [ 2.3593, -2.6240],\n",
      "        [ 0.5073, -0.5444],\n",
      "        [ 1.9065, -2.0254]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9810, 0.0190],\n",
      "        [0.9932, 0.0068],\n",
      "        [0.7411, 0.2589],\n",
      "        [0.9808, 0.0192]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 2.5440, -2.4192],\n",
      "        [ 1.4686, -2.1948],\n",
      "        [ 0.0056,  0.1035],\n",
      "        [-1.0818,  1.3387]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9931, 0.0069],\n",
      "        [0.9750, 0.0250],\n",
      "        [0.4755, 0.5245],\n",
      "        [0.0816, 0.9184]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-1.5688,  2.3112],\n",
      "        [ 1.7358, -2.1895],\n",
      "        [ 2.5431, -2.8881],\n",
      "        [ 2.2104, -3.1171]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0202, 0.9798],\n",
      "        [0.9806, 0.0194],\n",
      "        [0.9956, 0.0044],\n",
      "        [0.9952, 0.0048]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 1], device='cuda:0') tensor([[ 1.3507, -1.3575],\n",
      "        [ 1.0860, -0.5130],\n",
      "        [ 2.1568, -1.9786],\n",
      "        [ 1.8708, -1.7838]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9375, 0.0625],\n",
      "        [0.8319, 0.1681],\n",
      "        [0.9843, 0.0157],\n",
      "        [0.9748, 0.0252]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "280\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-0.9624,  0.7045],\n",
      "        [-1.7072,  1.8596],\n",
      "        [ 1.9190, -2.7963],\n",
      "        [ 2.1257, -2.6088]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1588, 0.8412],\n",
      "        [0.0275, 0.9725],\n",
      "        [0.9911, 0.0089],\n",
      "        [0.9913, 0.0087]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 2.0041, -2.4068],\n",
      "        [ 2.2866, -3.0173],\n",
      "        [-0.6751,  1.5318],\n",
      "        [ 1.0123, -0.9856]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9880, 0.0120],\n",
      "        [0.9951, 0.0049],\n",
      "        [0.0991, 0.9009],\n",
      "        [0.8806, 0.1194]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 1], device='cuda:0') tensor([[ 2.4208, -2.6410],\n",
      "        [ 2.1093, -2.5035],\n",
      "        [ 0.0237,  0.0888],\n",
      "        [-0.7734,  1.3109]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9937, 0.0063],\n",
      "        [0.9902, 0.0098],\n",
      "        [0.4837, 0.5163],\n",
      "        [0.1106, 0.8894]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 1.7088, -2.3290],\n",
      "        [ 2.4264, -2.7052],\n",
      "        [-1.3708,  1.2760],\n",
      "        [-1.9682,  2.2181]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9827, 0.0173],\n",
      "        [0.9941, 0.0059],\n",
      "        [0.0662, 0.9338],\n",
      "        [0.0150, 0.9850]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 1], device='cuda:0') tensor([[-0.5920,  1.2819],\n",
      "        [ 1.6934, -1.6251],\n",
      "        [-1.2291,  1.8108],\n",
      "        [-0.0140,  0.7363]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1331, 0.8669],\n",
      "        [0.9651, 0.0349],\n",
      "        [0.0457, 0.9543],\n",
      "        [0.3208, 0.6792]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 1.4206, -1.4721],\n",
      "        [ 2.3734, -2.5867],\n",
      "        [-1.7594,  2.6912],\n",
      "        [-2.0698,  2.9937]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9475, 0.0525],\n",
      "        [0.9930, 0.0070],\n",
      "        [0.0115, 0.9885],\n",
      "        [0.0063, 0.9937]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-2.2831,  3.1340],\n",
      "        [-2.1673,  2.6561],\n",
      "        [ 1.3193, -1.6511],\n",
      "        [-0.6825,  1.1916]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0044, 0.9956],\n",
      "        [0.0080, 0.9920],\n",
      "        [0.9512, 0.0488],\n",
      "        [0.1331, 0.8669]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-2.4202,  3.4069],\n",
      "        [-2.4112,  3.2045],\n",
      "        [ 2.2083, -2.4996],\n",
      "        [-0.9305,  1.2816]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0029, 0.9971],\n",
      "        [0.0036, 0.9964],\n",
      "        [0.9911, 0.0089],\n",
      "        [0.0987, 0.9013]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 1.9207, -2.1865],\n",
      "        [ 1.0732, -1.0400],\n",
      "        [-2.2223,  2.7473],\n",
      "        [-2.6400,  3.1718]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9838, 0.0162],\n",
      "        [0.8922, 0.1078],\n",
      "        [0.0069, 0.9931],\n",
      "        [0.0030, 0.9970]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.8526, -2.1388],\n",
      "        [ 2.1506, -2.8507],\n",
      "        [-1.2310,  1.4423],\n",
      "        [ 0.1124,  0.0876]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9819, 0.0181],\n",
      "        [0.9933, 0.0067],\n",
      "        [0.0646, 0.9354],\n",
      "        [0.5062, 0.4938]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "290\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 0, 0], device='cuda:0') tensor([[-1.4549,  1.5419],\n",
      "        [-1.1092,  1.0856],\n",
      "        [ 1.5907, -2.1651],\n",
      "        [-2.3100,  2.8350]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0476, 0.9524],\n",
      "        [0.1002, 0.8998],\n",
      "        [0.9772, 0.0228],\n",
      "        [0.0058, 0.9942]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 0, 0, 0], device='cuda:0') tensor([[-1.2594,  2.2375],\n",
      "        [ 1.1799, -1.0304],\n",
      "        [-1.5512,  1.6652],\n",
      "        [ 1.3585, -1.6828]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0294, 0.9706],\n",
      "        [0.9012, 0.0988],\n",
      "        [0.0386, 0.9614],\n",
      "        [0.9544, 0.0456]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([1, 1, 1, 1], device='cuda:0') tensor([[-2.1466,  3.1531],\n",
      "        [-1.6811,  2.5701],\n",
      "        [-2.7179,  2.9705],\n",
      "        [-1.3766,  2.5883]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0050, 0.9950],\n",
      "        [0.0140, 0.9860],\n",
      "        [0.0034, 0.9966],\n",
      "        [0.0186, 0.9814]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.0195, -2.5563],\n",
      "        [ 1.2258, -1.2372],\n",
      "        [ 2.1888, -2.3889],\n",
      "        [ 1.9965, -2.6925]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9898, 0.0102],\n",
      "        [0.9215, 0.0785],\n",
      "        [0.9898, 0.0102],\n",
      "        [0.9909, 0.0091]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 1], device='cuda:0') tensor([[ 2.1681, -2.1588],\n",
      "        [-2.1181,  3.3174],\n",
      "        [-2.3389,  3.3617],\n",
      "        [-1.4875,  2.2378]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9870, 0.0130],\n",
      "        [0.0043, 0.9957],\n",
      "        [0.0033, 0.9967],\n",
      "        [0.0235, 0.9765]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 1, 1], device='cuda:0') tensor([[ 0.6796, -1.0066],\n",
      "        [ 0.3629, -0.1177],\n",
      "        [-0.4334,  1.2355],\n",
      "        [-1.5773,  2.0971]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8437, 0.1563],\n",
      "        [0.6179, 0.3821],\n",
      "        [0.1586, 0.8414],\n",
      "        [0.0247, 0.9753]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.7414, -2.1314],\n",
      "        [ 1.4899, -1.1912],\n",
      "        [ 2.0754, -2.4150],\n",
      "        [ 2.1456, -2.5913]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9796, 0.0204],\n",
      "        [0.9359, 0.0641],\n",
      "        [0.9889, 0.0111],\n",
      "        [0.9913, 0.0087]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 1, 1, 0], device='cuda:0') tensor([[ 1.6306, -2.4743],\n",
      "        [-0.0278,  0.1707],\n",
      "        [ 0.7151, -0.7256],\n",
      "        [ 1.7246, -2.4044]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9838, 0.0162],\n",
      "        [0.4505, 0.5495],\n",
      "        [0.8086, 0.1914],\n",
      "        [0.9842, 0.0158]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 2.2647, -2.6056],\n",
      "        [-0.5766,  0.9270],\n",
      "        [ 1.5800, -1.8612],\n",
      "        [-2.5433,  3.3771]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9924, 0.0076],\n",
      "        [0.1819, 0.8181],\n",
      "        [0.9690, 0.0310],\n",
      "        [0.0027, 0.9973]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Pre-classification out torch.Size([4, 768])\n",
      "Post-classification out torch.Size([4, 2])\n",
      "tensor([0, 0, 0, 0], device='cuda:0') tensor([[ 1.1836, -0.9694],\n",
      "        [ 1.9299, -2.4728],\n",
      "        [ 1.8407, -1.9398],\n",
      "        [ 1.5391, -1.8626]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.8960, 0.1040],\n",
      "        [0.9879, 0.0121],\n",
      "        [0.9777, 0.0223],\n",
      "        [0.9678, 0.0322]], device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa56c53e3a0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = t.device('cuda:0' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = t.optim.Adam(my_bert.parameters(), lr = 1e-5)\n",
    "criterion = t.nn.CrossEntropyLoss()\n",
    "\n",
    "steps = 300\n",
    "batches = extract_batches(dataset_train)\n",
    "\n",
    "my_bert.train()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(steps):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    X,y = batches[i]\n",
    "    X = X.cuda(device)\n",
    "    y = y.cuda(device)\n",
    "    pred_y = my_bert(X)[1]\n",
    "\n",
    "    pred_y = pred_y.softmax(dim=1).cuda(device)\n",
    "    \n",
    "    print('Actual', y)\n",
    "    print('Predicted', pred_y)\n",
    "\n",
    "    loss = criterion(pred_y, y)\n",
    "    loss.backward()\n",
    "    losses.append(loss.detach())\n",
    "    optimizer.step()\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training from Scratch on Masked Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torchtext\n",
    "\n",
    "data_train = torchtext.datasets.WikiText2(root='.data',split='train')\n",
    "data_test = torchtext.datasets.WikiText2(root='.data',split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = []\n",
    "for d in data_train:\n",
    "    dataset_train.append(d)\n",
    "\n",
    "dataset_test = []\n",
    "for d in data_test:\n",
    "    dataset_test.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_batches(dataset, batch_size=4, max_seq_length=512):\n",
    "\n",
    "    dataset.sort(key=lambda x: len(x[1]))\n",
    "\n",
    "    number_of_batches = math.ceil(len(dataset) / batch_size)\n",
    "\n",
    "    dataset = np.array_split(dataset, number_of_batches)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for batch in dataset:\n",
    "        tokens = tokenizer(list(batch))\n",
    "\n",
    "        tokens = tokens[\"input_ids\"]\n",
    "\n",
    "        tokens = [line[:max_seq_length] for line in tokens]\n",
    "        tokens = [line if len(line) == max_seq_length else line + (max_seq_length - len(line))*[tokenizer.pad_token_id] for line in tokens]\n",
    "\n",
    "        X = t.tensor(tokens)\n",
    "\n",
    "        batches.append((X,y))\n",
    "\n",
    "    random.shuffle(batches)\n",
    "    return batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
