{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/mlab/days/w3d3/rl_env/lib/python3.8/site-packages\")\n",
    "sys.path.append(\"/home/ubuntu/mlab/days/w3d3/rl_env/lib/python3.9/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import Video\n",
    "from video_recorder import VideoRecorder\n",
    "import torch as t\n",
    "from time import ctime\n",
    "from datetime import datetime\n",
    "import rl_tests\n",
    "from collections import deque\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_choice(\n",
    "    env: gym.Env, eps: float, net: t.nn.Module, obs: t.Tensor, device: str\n",
    "):\n",
    "    \n",
    "    u = t.rand(1).item()\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    if u < eps:\n",
    "        return t.randint(high = num_actions, size = (1,)).item()\n",
    "    \n",
    "    else:\n",
    "        with t.no_grad():\n",
    "            q = net(obs)\n",
    "            return t.argmax(q).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env_name, eps: float, device: str, save_video: bool, show_video: bool, verbose: bool, make_atari_wrapper=True):\n",
    "    with t.no_grad():\n",
    "        env = gym.make(env_name)\n",
    "        if make_atari_wrapper:\n",
    "            env = AtariWrapper(env)\n",
    "        \n",
    "        if save_video:\n",
    "            now = datetime.now()\n",
    "            time = now.strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            video_string = f\"videos/env_{time}.mp4\"\n",
    "            recorder = VideoRecorder(env, video_string, enabled=save_video)\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        states = 0\n",
    "\n",
    "        while not done:\n",
    "            states += 1\n",
    "\n",
    "            if save_video:\n",
    "                recorder.capture_frame()\n",
    "            if show_video:\n",
    "                show_state(env)\n",
    "\n",
    "            obs = state\n",
    "\n",
    "            # take an epsilon-greedy action\n",
    "            action = make_choice(env, eps, model, obs, device) \n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "        if verbose: print(states)\n",
    "        if verbose: print(f\"total reward: {total_reward}\")\n",
    "        if save_video:\n",
    "            if verbose: print(f\"Saving video as {video_string}\")\n",
    "            recorder.close()\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, env_name, epsilon, eps_end, eps_end_idx, eval_eps=0.05, add_atari_wrapper=True):\n",
    "    env = gym.make(env_name)\n",
    "    env = AtariWrapper(env)\n",
    "\n",
    "    replay_buffer = deque()\n",
    "    obs_new = env.reset()\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=adam_lr)\n",
    "    model.train()\n",
    "    \n",
    "    d_eps = (eps_end - epsilon) / eps_end_idx\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        if step % eval_interval == 0:\n",
    "            print(f\"Logging hundredth step: {step}\")\n",
    "            \n",
    "            total_reward = evaluate(model, env_name, eval_eps, device, save_video=False, show_video=False, verbose=False)\n",
    "            print(f\"Evaluating: total reward {total_reward}\")\n",
    "        \n",
    "        obs = obs_new\n",
    "\n",
    "        # take an epsilon-greedy action\n",
    "        action = make_choice(env, epsilon, model, obs, device)\n",
    "        obs_new, reward, done, _ = env.step(action)\n",
    "\n",
    "        # store transition in replay buffer\n",
    "        replay_buffer.append((obs, action, reward, done, obs_new))\n",
    "\n",
    "        # sample random minibatches from replay buffer\n",
    "        sample_indices = t.randint(high=len(replay_buffer), size=(batch_size,))\n",
    "        loss = 0.0\n",
    "        \n",
    "        if step % train_freq == 0:\n",
    "            # print(\"Doing optimization, step:\", step)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            for i in sample_indices:\n",
    "                sample_obs, sample_action, sample_reward, sample_done, sample_obs_new = replay_buffer[i]\n",
    "\n",
    "                y = sample_reward\n",
    "\n",
    "                if not sample_done:\n",
    "                    with t.no_grad():\n",
    "                        y += gamma * t.max(model(sample_obs_new))\n",
    "\n",
    "                # loss += criterion(model(obs)[action], y)\n",
    "                q_vals = model(sample_obs)[0]\n",
    "                loss += (q_vals[sample_action]-y)**2\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # limit the number of experiences in the buffer\n",
    "        if len(replay_buffer) > exp_buffer_size:\n",
    "            replay_buffer.popleft()\n",
    "        if done:\n",
    "            # print('Done', step)\n",
    "            obs_new = env.reset()\n",
    "        if step < eps_end_idx:\n",
    "            epsilon += d_eps\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BreakoutNoFrameSkip-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class CNN(t.nn.Module):\n",
    "    def __init__(self, obs_n_channels, n_action_space, device):\n",
    "        super().__init__()\n",
    "        self.device=device\n",
    "        self.model = t.nn.Sequential(\n",
    "            t.nn.Conv2d(obs_n_channels, 32, 8, stride=4),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Conv2d(32, 64, 4, stride=2),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Conv2d(64, 64, 3, stride=1),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Flatten(),\n",
    "            t.nn.Linear(3136, n_action_space)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(\n",
    "            rearrange(t.tensor(x, dtype=t.float32, device=self.device).unsqueeze(0), 'a b c d -> a d b c')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from days.atari_wrappers import AtariWrapper\n",
    "\n",
    "env_name=\"BreakoutNoFrameskip-v0\"\n",
    "num_steps=1_000_000\n",
    "adam_lr=3e-5\n",
    "gamma=0.99\n",
    "exp_buffer_size=100_000\n",
    "train_freq = 4\n",
    "batch_size = 32\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_end_idx = num_steps\n",
    "eval_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v0\")\n",
    "env = AtariWrapper(env)\n",
    "state = env.reset()\n",
    "cnn = CNN(state.shape[-1], env.action_space.n, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging hundredth step: 0\n",
      "Evaluating: total reward 2.0\n",
      "Logging hundredth step: 100\n",
      "Evaluating: total reward 2.0\n",
      "Logging hundredth step: 200\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 300\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 400\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 500\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 600\n",
      "Evaluating: total reward 1.0\n",
      "Logging hundredth step: 700\n",
      "Evaluating: total reward 1.0\n",
      "Logging hundredth step: 800\n",
      "Evaluating: total reward 2.0\n",
      "Logging hundredth step: 900\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 1000\n",
      "Evaluating: total reward 2.0\n",
      "Logging hundredth step: 1100\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 1200\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 1300\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 1400\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 1500\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 1600\n",
      "Evaluating: total reward 2.0\n",
      "Logging hundredth step: 1700\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 1800\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 1900\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2000\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2100\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2200\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2300\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2400\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2500\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2600\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2700\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2800\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 2900\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 3000\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 3100\n",
      "Evaluating: total reward 1.0\n",
      "Logging hundredth step: 3200\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 3300\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 3400\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 3500\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 3600\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 3700\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 3800\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 3900\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 4000\n",
      "Evaluating: total reward 0.0\n",
      "Logging hundredth step: 4100\n",
      "Evaluating: total reward 2.0\n",
      "Logging hundredth step: 4200\n",
      "Evaluating: total reward 0.0\n"
     ]
    }
   ],
   "source": [
    "cnn = train_model(cnn, env_name, epsilon=eps_start, eps_end=eps_end, eps_end_idx=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    states += 1\n",
    "    # if record:\n",
    "        # recorder.capture_frame()\n",
    "    # else:  \n",
    "    show_state(env)\n",
    "    # if record:\n",
    "    #     recorder.capture_frame()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample()) # Take a random action\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a131218803ea04de256dcf68d0863f351f9c21059ac7f8146793fa89223f504a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
